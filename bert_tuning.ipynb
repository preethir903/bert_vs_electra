{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "bert_tuning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/preethir903/w266_final_project/blob/main/bert_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "LVodDdueGzZp",
        "outputId": "d6f3b54c-862f-43eb-bbdd-b9b630ff593c"
      },
      "source": [
        "from google.colab import files \n",
        "uploaded = files.upload()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ae9dd430-329a-4f08-9949-6f91adcb96e2\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ae9dd430-329a-4f08-9949-6f91adcb96e2\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving full_df.csv to full_df.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPk9y36Nh7EE",
        "outputId": "3c564d4e-01a9-4134-e6ea-f85bf41327d5"
      },
      "source": [
        "# see if full_df is present in the data\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "full_df.csv  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQriZ5rSKntq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f16b685-7f48-4605-a555-61b5276c4af1"
      },
      "source": [
        "# A dependency of the preprocessing for BERT inputs\n",
        "!pip install -q tensorflow-text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 4.3MB 8.0MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "767WYwdzJsV7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22228fa9-8f2c-4539-f092-971cc54bfd3d"
      },
      "source": [
        "pip install -q tf-models-official"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.6MB 4.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 645kB 41.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 102kB 15.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 8.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 38.2MB 78kB/s \n",
            "\u001b[K     |████████████████████████████████| 174kB 53.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 686kB 43.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2MB 48.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 358kB 50.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 9.0MB/s \n",
            "\u001b[?25h  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FsroAKE9I3-3",
        "outputId": "39e76409-b257-4b26-f547-26040179990b"
      },
      "source": [
        "pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/43/cfe4ee779bbd6a678ac6a97c5a5cdeb03c35f9eaebbb9720b036680f9a2d/transformers-4.6.1-py3-none-any.whl (2.2MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3MB 8.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Collecting huggingface-hub==0.0.8\n",
            "  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 49.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.0.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 42.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Installing collected packages: huggingface-hub, tokenizers, sacremoses, transformers\n",
            "Successfully installed huggingface-hub-0.0.8 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.6.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6_fT1VgIN6o"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "\n",
        "from official.nlp import optimization  # to create AdamW optmizer\n",
        "#Load the BERT Classifier and Tokenizer alıng with Input modules\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "from transformers import InputExample, InputFeatures\n",
        "\n",
        "from keras import backend as K\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "id": "QNZGnZSDH0rE",
        "outputId": "3d63a1b2-166e-422e-8739-ad238213ddf9"
      },
      "source": [
        "# Get full dataset\n",
        "df = pd.read_csv('full_df.csv', index_col = 0)\n",
        "df['label'] = pd.Categorical(df['ideology'])\n",
        "df['label'] = df.label.cat.codes\n",
        "df.head(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ideology</th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Liberal</td>\n",
              "      <td>Apart from the legal rights , the fact that le...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Liberal</td>\n",
              "      <td>Today , a sizable and growing number of indivi...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  ideology                                           sentence  label\n",
              "0  Liberal  Apart from the legal rights , the fact that le...      1\n",
              "1  Liberal  Today , a sizable and growing number of indivi...      1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_yysyIcIIzc"
      },
      "source": [
        "map_name_to_handle = {\n",
        "    'bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n",
        "    'bert_en_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n",
        "    'bert_multi_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n",
        "    'albert_en_base':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_base/2',\n",
        "    'electra_small':\n",
        "        'https://tfhub.dev/google/electra_small/2',\n",
        "    'electra_base':\n",
        "        'https://tfhub.dev/google/electra_base/2',\n",
        "    'experts_pubmed':\n",
        "        'https://tfhub.dev/google/experts/bert/pubmed/2',\n",
        "    'experts_wiki_books':\n",
        "        'https://tfhub.dev/google/experts/bert/wiki_books/2',\n",
        "    'talking-heads_base':\n",
        "        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n",
        "}\n",
        "\n",
        "map_model_to_preprocess = {\n",
        "    'bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'bert_en_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'bert_multi_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\n",
        "    'albert_en_base':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n",
        "    'electra_small':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'electra_base':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'experts_pubmed':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'experts_wiki_books':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'talking-heads_base':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "}\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6jEPosnqbsm"
      },
      "source": [
        "# plots\n",
        "\n",
        "def plot_loss(history): \n",
        "  history_dict = history.history\n",
        "  print(history_dict.keys())\n",
        "\n",
        "  acc = history_dict['binary_accuracy']\n",
        "  val_acc = history_dict['val_binary_accuracy']\n",
        "  loss = history_dict['loss']\n",
        "  val_loss = history_dict['val_loss']\n",
        "\n",
        "  epochs = range(1, len(acc) + 1)\n",
        "  fig = plt.figure(figsize=(10, 6))\n",
        "  fig.tight_layout()\n",
        "\n",
        "  plt.subplot(2, 1, 1)\n",
        "  # \"bo\" is for \"blue dot\"\n",
        "  plt.plot(epochs, loss, 'r', label='Training loss')\n",
        "  # b is for \"solid blue line\"\n",
        "  plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "  plt.title('Training and validation loss')\n",
        "  # plt.xlabel('Epochs')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend()\n",
        "\n",
        "  plt.subplot(2, 1, 2)\n",
        "  plt.plot(epochs, acc, 'r', label='Training acc')\n",
        "  plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "  plt.title('Training and validation accuracy')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.legend(loc='lower right')\n",
        "\n",
        "def plot_model_structure(classifier_model):\n",
        "  # model structure\n",
        "  tf.keras.utils.plot_model(classifier_model)\n",
        "  #shapes of inputs\n",
        "  tf.keras.utils.plot_model(classifier_model, show_shapes=True, dpi=58)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7zt5DzZQ6Fq"
      },
      "source": [
        "seed = 1\n",
        "\n",
        "def tf_format_data(df, batch_size, seed = seed): \n",
        "  features = df.sentence.values\n",
        "  labels = df.label.values\n",
        "  dataset = tf.data.Dataset.from_tensor_slices( (features, labels) )\n",
        "  raw_df = dataset.shuffle(len(df), seed = seed).batch(batch_size)\n",
        "  AUTOTUNE = tf.data.AUTOTUNE\n",
        "  format_df = raw_df.cache().prefetch(buffer_size=AUTOTUNE) \n",
        "  return format_df\n",
        "\n",
        "def stratified_10_fold_split(df, batch_size, fold = 0, val = True, seed = seed, tf_format = True): \n",
        "  df = df.sample(frac=1, random_state = seed) # shuffle data\n",
        "  label_1 = df[df.label == df.label.unique()[0]].copy() # stratify data\n",
        "  label_2 = df[df.label == df.label.unique()[1]].copy()\n",
        "  # split train / test\n",
        "  split = int(len(df) / (10 * 2)) \n",
        "  test1 = label_1.iloc[fold * split : fold * split + split]\n",
        "  test2 = label_2.iloc[fold * split : fold * split + split]\n",
        "  train1 = label_1[~label_1.index.isin(test1.index)]\n",
        "  train2 = label_2[~label_2.index.isin(test2.index)]\n",
        "  if val: \n",
        "    # then split train into train / val\n",
        "    split = int(len(train1) / (9))\n",
        "    val1 = train1.iloc[fold * split : fold * split + split]\n",
        "    val2 = train2.iloc[fold * split : fold * split + split]\n",
        "    train1 = train1[~train1.index.isin(val1.index)]\n",
        "    train2 = train2[~train2.index.isin(val2.index)]\n",
        "    # concatenate both labels \n",
        "    train, val, test = pd.concat([train1,train2]),pd.concat([val1,val2]),pd.concat([test1,test2])\n",
        "    print('Train ', len(train)) \n",
        "    print('Val ', len(val)) \n",
        "    print('Test ', len(test)) \n",
        "    return tf_format_data(train, batch_size), tf_format_data(val, batch_size), tf_format_data(test, batch_size) \n",
        "  else: \n",
        "    # split only to train test, no val\n",
        "    train, test = pd.concat([train1,train2]),pd.concat([test1,test2]) \n",
        "    print('Train ', len(train)) \n",
        "    print('Test ', len(test)) \n",
        "    if tf_format: \n",
        "      return tf_format_data(train, batch_size), tf_format_data(test, batch_size) \n",
        "    else: \n",
        "      return train, test   \n",
        "\n",
        "\n",
        "def build_classifier_model():\n",
        "  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
        "  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
        "  encoder_inputs = preprocessing_layer(text_input)\n",
        "  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
        "  outputs = encoder(encoder_inputs)\n",
        "  net = outputs['pooled_output']\n",
        "  net = tf.keras.layers.Dropout(0.1)(net)\n",
        "  net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n",
        "  #net = tf.keras.layers.Dense(1, activation='tanh', name='classifier')(net)\n",
        "  return tf.keras.Model(text_input, net)\n",
        "\n",
        "def load_bert_to_keras(bert_model_name, do_fine_tuning):\n",
        "    #load pre-trained model to keras\n",
        "    tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
        "    tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n",
        "    bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)\n",
        "    bert_model = hub.KerasLayer(tfhub_handle_encoder)\n",
        "    # build classifier model\n",
        "    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
        "    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
        "    encoder_inputs = preprocessing_layer(text_input)\n",
        "    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=do_fine_tuning, name='BERT_encoder')\n",
        "    outputs = encoder(encoder_inputs)\n",
        "    return outputs['pooled_output'], text_input"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnEcf3fUeiyO"
      },
      "source": [
        "# put all the above in one function to initialize for each bert mode\n",
        "\n",
        "def bert_10_fold(epochs, init_lr, batch_size, create_classifier, bert_model_name = 'electra_base', do_fine_tuning = True, folds = 10): \n",
        "  print(bert_model_name)\n",
        "  # get average accuracy on test (k-fold)\n",
        "  accuracies = []\n",
        "  for fold in range(folds): \n",
        "    print('FOLD ', fold)\n",
        "    #load data - seed set upstream to 0 \n",
        "    train, test = stratified_10_fold_split(df, fold = fold, val = False, batch_size = batch_size) \n",
        "    # load bert model to keras\n",
        "    net, text_input = load_bert_to_keras(bert_model_name, do_fine_tuning)\n",
        "    # create rest of the layers / classifier\n",
        "    classifier_model = create_classifier(net, text_input)\n",
        "\n",
        "    #loss function & evaluation\n",
        "    loss = tf.keras.losses.BinaryCrossentropy()\n",
        "    metrics = tf.metrics.BinaryAccuracy()\n",
        "\n",
        "    # optimizer\n",
        "    steps_per_epoch = tf.data.experimental.cardinality(train).numpy()\n",
        "    num_train_steps = steps_per_epoch * epochs\n",
        "    num_warmup_steps = int(0.1*num_train_steps)\n",
        "    optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                              num_train_steps=num_train_steps,\n",
        "                                              num_warmup_steps=num_warmup_steps,\n",
        "                                              optimizer_type='adamw')\n",
        "    # load BERT model\n",
        "    classifier_model.compile(optimizer=optimizer,\n",
        "                            loss=loss,\n",
        "                            metrics=metrics)\n",
        "    \n",
        "    # fit model and calculate accuracy on test    \n",
        "    history = classifier_model.fit(x=train, validation_data=test, epochs=epochs)\n",
        "    loss, accuracy = classifier_model.evaluate(test)\n",
        "    accuracies.append(accuracy)\n",
        "\n",
        "  return {bert_model_name: accuracies}, history, classifier_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltrmMeL5pTJK"
      },
      "source": [
        "# bert baseline, no fine tuning\n",
        "bert_model_name = 'bert_en_uncased_L-12_H-768_A-12'\n",
        "epochs = 5\n",
        "init_lr = 3e-5\n",
        "batch_size = 32\n",
        "do_fine_tuning = False\n",
        "folds = 2\n",
        "\n",
        "def create_layers(net, text_input): \n",
        "  net = tf.keras.layers.Dropout(0.1)(net)\n",
        "  net = tf.keras.layers.Dense(1, activation='sigmoid', name='classifier')(net)\n",
        "  return tf.keras.Model(text_input, net)\n",
        "\n",
        "results, history, classifier_model = bert_10_fold(epochs=epochs, \n",
        "                                                  init_lr=init_lr, \n",
        "                                                  batch_size = batch_size, \n",
        "                                                  do_fine_tuning = do_fine_tuning, \n",
        "                                                  create_classifier = create_layers, \n",
        "                                                  bert_model_name = bert_model_name, \n",
        "                                                  folds = folds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5ylFJpCo8TV",
        "outputId": "56b3b070-dfd5-4f36-a7ce-5972f9da56dd"
      },
      "source": [
        "# electra baseline, no fine tuning\n",
        "bert_model_name = 'electra_base'\n",
        "epochs = 5\n",
        "init_lr = 3e-5\n",
        "batch_size = 32\n",
        "do_fine_tuning = False\n",
        "folds = 2\n",
        "\n",
        "def create_layers(net, text_input): \n",
        "  net = tf.keras.layers.Dropout(0.1)(net)\n",
        "  net = tf.keras.layers.Dense(1, activation='sigmoid', name='classifier')(net)\n",
        "  return tf.keras.Model(text_input, net)\n",
        "\n",
        "results, history, classifier_model = bert_10_fold(epochs=epochs, \n",
        "                                                  init_lr=init_lr, \n",
        "                                                  batch_size = batch_size, \n",
        "                                                  do_fine_tuning = do_fine_tuning, \n",
        "                                                  create_classifier = create_layers, \n",
        "                                                  bert_model_name = bert_model_name, \n",
        "                                                  folds = folds)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "electra_base\n",
            "FOLD  0\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "96/96 [==============================] - 28s 220ms/step - loss: 0.6994 - binary_accuracy: 0.5199 - val_loss: 0.7052 - val_binary_accuracy: 0.5000\n",
            "Epoch 2/5\n",
            "96/96 [==============================] - 20s 213ms/step - loss: 0.6944 - binary_accuracy: 0.5144 - val_loss: 0.7037 - val_binary_accuracy: 0.4588\n",
            "Epoch 3/5\n",
            "96/96 [==============================] - 20s 214ms/step - loss: 0.6925 - binary_accuracy: 0.5259 - val_loss: 0.7033 - val_binary_accuracy: 0.4618\n",
            "Epoch 4/5\n",
            "96/96 [==============================] - 20s 213ms/step - loss: 0.6916 - binary_accuracy: 0.5197 - val_loss: 0.7031 - val_binary_accuracy: 0.4676\n",
            "Epoch 5/5\n",
            "96/96 [==============================] - 20s 213ms/step - loss: 0.6916 - binary_accuracy: 0.5349 - val_loss: 0.7031 - val_binary_accuracy: 0.4706\n",
            "11/11 [==============================] - 2s 185ms/step - loss: 0.7031 - binary_accuracy: 0.4706\n",
            "FOLD  1\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "96/96 [==============================] - 28s 220ms/step - loss: 0.7327 - binary_accuracy: 0.5054 - val_loss: 0.7041 - val_binary_accuracy: 0.5147\n",
            "Epoch 2/5\n",
            "96/96 [==============================] - 20s 213ms/step - loss: 0.7067 - binary_accuracy: 0.4968 - val_loss: 0.6937 - val_binary_accuracy: 0.5176\n",
            "Epoch 3/5\n",
            "96/96 [==============================] - 20s 213ms/step - loss: 0.7040 - binary_accuracy: 0.5046 - val_loss: 0.6914 - val_binary_accuracy: 0.5118\n",
            "Epoch 4/5\n",
            "96/96 [==============================] - 20s 213ms/step - loss: 0.6988 - binary_accuracy: 0.5035 - val_loss: 0.6906 - val_binary_accuracy: 0.4971\n",
            "Epoch 5/5\n",
            "96/96 [==============================] - 20s 213ms/step - loss: 0.7028 - binary_accuracy: 0.4969 - val_loss: 0.6905 - val_binary_accuracy: 0.5029\n",
            "11/11 [==============================] - 2s 185ms/step - loss: 0.6905 - binary_accuracy: 0.5029\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXpCzUs_qCGd",
        "outputId": "932b1cfb-d580-4d93-fe55-b86dc202f2b0"
      },
      "source": [
        "# bert baseline, fine tuning\n",
        "bert_model_name = 'bert_en_uncased_L-12_H-768_A-12'\n",
        "epochs = 3\n",
        "init_lr = 3e-5\n",
        "batch_size = 32\n",
        "do_fine_tuning = True\n",
        "folds = 2\n",
        "\n",
        "def create_layers(net, text_input): \n",
        "  net = tf.keras.layers.Dropout(0.1)(net)\n",
        "  net = tf.keras.layers.Dense(1, activation='sigmoid', name='classifier')(net)\n",
        "  return tf.keras.Model(text_input, net)\n",
        "\n",
        "results, history, classifier_model = bert_10_fold(epochs=epochs, \n",
        "                                                  init_lr=init_lr, \n",
        "                                                  batch_size = batch_size, \n",
        "                                                  do_fine_tuning = do_fine_tuning, \n",
        "                                                  create_classifier = create_layers, \n",
        "                                                  bert_model_name = bert_model_name, \n",
        "                                                  folds = folds)\n",
        "\n",
        "print('Average error ', np.mean(results[bert_model_name]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bert_en_uncased_L-12_H-768_A-12\n",
            "FOLD  0\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "96/96 [==============================] - 68s 550ms/step - loss: 0.7003 - binary_accuracy: 0.5407 - val_loss: 0.6735 - val_binary_accuracy: 0.5618\n",
            "Epoch 2/3\n",
            "96/96 [==============================] - 52s 542ms/step - loss: 0.6348 - binary_accuracy: 0.6501 - val_loss: 0.7007 - val_binary_accuracy: 0.5941\n",
            "Epoch 3/3\n",
            "96/96 [==============================] - 52s 542ms/step - loss: 0.4998 - binary_accuracy: 0.7616 - val_loss: 0.7166 - val_binary_accuracy: 0.6176\n",
            "11/11 [==============================] - 2s 186ms/step - loss: 0.7166 - binary_accuracy: 0.6176\n",
            "FOLD  1\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "96/96 [==============================] - 66s 549ms/step - loss: 0.6954 - binary_accuracy: 0.5302 - val_loss: 0.6471 - val_binary_accuracy: 0.6471\n",
            "Epoch 2/3\n",
            "96/96 [==============================] - 52s 542ms/step - loss: 0.6176 - binary_accuracy: 0.6559 - val_loss: 0.6424 - val_binary_accuracy: 0.6618\n",
            "Epoch 3/3\n",
            "96/96 [==============================] - 52s 542ms/step - loss: 0.4643 - binary_accuracy: 0.7823 - val_loss: 0.7318 - val_binary_accuracy: 0.6441\n",
            "11/11 [==============================] - 2s 185ms/step - loss: 0.7318 - binary_accuracy: 0.6441\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2CGIw3pRwF94",
        "outputId": "16565f33-9d7a-4c0e-997a-d0ceacf3c138"
      },
      "source": [
        "# bert baseline, fine tuning\n",
        "bert_model_name = 'bert_en_uncased_L-12_H-768_A-12'\n",
        "epochs = 4\n",
        "init_lr = 3e-5\n",
        "batch_size = 32\n",
        "do_fine_tuning = True\n",
        "folds = 2\n",
        "\n",
        "def create_layers(net, text_input): \n",
        "  net = tf.keras.layers.Dropout(0.1)(net)\n",
        "  net = tf.keras.layers.Dense(1, activation='sigmoid', name='classifier')(net)\n",
        "  return tf.keras.Model(text_input, net)\n",
        "\n",
        "results, history, classifier_model = bert_10_fold(epochs=epochs, \n",
        "                                                  init_lr=init_lr, \n",
        "                                                  batch_size = batch_size, \n",
        "                                                  do_fine_tuning = do_fine_tuning, \n",
        "                                                  create_classifier = create_layers, \n",
        "                                                  bert_model_name = bert_model_name, \n",
        "                                                  folds = folds)\n",
        "\n",
        "print('Average error ', np.mean(results[bert_model_name]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bert_en_uncased_L-12_H-768_A-12\n",
            "FOLD  0\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "96/96 [==============================] - 67s 550ms/step - loss: 0.7049 - binary_accuracy: 0.5200 - val_loss: 0.6754 - val_binary_accuracy: 0.5647\n",
            "Epoch 2/4\n",
            "96/96 [==============================] - 52s 543ms/step - loss: 0.6378 - binary_accuracy: 0.6360 - val_loss: 0.6734 - val_binary_accuracy: 0.6000\n",
            "Epoch 3/4\n",
            "96/96 [==============================] - 52s 543ms/step - loss: 0.5116 - binary_accuracy: 0.7569 - val_loss: 0.7508 - val_binary_accuracy: 0.6265\n",
            "Epoch 4/4\n",
            "96/96 [==============================] - 52s 542ms/step - loss: 0.3748 - binary_accuracy: 0.8508 - val_loss: 0.8006 - val_binary_accuracy: 0.6265\n",
            "11/11 [==============================] - 2s 185ms/step - loss: 0.8006 - binary_accuracy: 0.6265\n",
            "FOLD  1\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "96/96 [==============================] - 66s 550ms/step - loss: 0.7321 - binary_accuracy: 0.5033 - val_loss: 0.6526 - val_binary_accuracy: 0.6235\n",
            "Epoch 2/4\n",
            "96/96 [==============================] - 52s 543ms/step - loss: 0.6526 - binary_accuracy: 0.6149 - val_loss: 0.6410 - val_binary_accuracy: 0.6412\n",
            "Epoch 3/4\n",
            "96/96 [==============================] - 52s 542ms/step - loss: 0.5203 - binary_accuracy: 0.7486 - val_loss: 0.6989 - val_binary_accuracy: 0.6294\n",
            "Epoch 4/4\n",
            "96/96 [==============================] - 52s 542ms/step - loss: 0.3749 - binary_accuracy: 0.8372 - val_loss: 0.7241 - val_binary_accuracy: 0.6647\n",
            "11/11 [==============================] - 2s 188ms/step - loss: 0.7241 - binary_accuracy: 0.6647\n",
            "Average error  0.645588219165802\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PlAvLumDvCIE",
        "outputId": "e7da6ac7-a2e9-4a74-ffe7-81c878d1c58d"
      },
      "source": [
        "# bert baseline, fine tuning\n",
        "bert_model_name = 'bert_en_uncased_L-12_H-768_A-12'\n",
        "epochs = 4\n",
        "init_lr = 2e-5\n",
        "batch_size = 32\n",
        "do_fine_tuning = True\n",
        "folds = 2\n",
        "\n",
        "def create_layers(net, text_input): \n",
        "  net = tf.keras.layers.Dropout(0.1)(net)\n",
        "  net = tf.keras.layers.Dense(1, activation='sigmoid', name='classifier')(net)\n",
        "  return tf.keras.Model(text_input, net)\n",
        "\n",
        "results, history, classifier_model = bert_10_fold(epochs=epochs, \n",
        "                                                  init_lr=init_lr, \n",
        "                                                  batch_size = batch_size, \n",
        "                                                  do_fine_tuning = do_fine_tuning, \n",
        "                                                  create_classifier = create_layers, \n",
        "                                                  bert_model_name = bert_model_name, \n",
        "                                                  folds = folds)\n",
        "\n",
        "print('Average error ', np.mean(results[bert_model_name]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bert_en_uncased_L-12_H-768_A-12\n",
            "FOLD  0\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "96/96 [==============================] - 67s 550ms/step - loss: 0.7524 - binary_accuracy: 0.5235 - val_loss: 0.6862 - val_binary_accuracy: 0.5441\n",
            "Epoch 2/4\n",
            "96/96 [==============================] - 52s 542ms/step - loss: 0.6646 - binary_accuracy: 0.5902 - val_loss: 0.6792 - val_binary_accuracy: 0.5882\n",
            "Epoch 3/4\n",
            "96/96 [==============================] - 52s 543ms/step - loss: 0.5710 - binary_accuracy: 0.7005 - val_loss: 0.7201 - val_binary_accuracy: 0.6000\n",
            "Epoch 4/4\n",
            "96/96 [==============================] - 52s 543ms/step - loss: 0.4784 - binary_accuracy: 0.7645 - val_loss: 0.7234 - val_binary_accuracy: 0.6235\n",
            "11/11 [==============================] - 2s 186ms/step - loss: 0.7234 - binary_accuracy: 0.6235\n",
            "FOLD  1\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "96/96 [==============================] - 67s 550ms/step - loss: 0.6996 - binary_accuracy: 0.5405 - val_loss: 0.6428 - val_binary_accuracy: 0.6412\n",
            "Epoch 2/4\n",
            "96/96 [==============================] - 52s 543ms/step - loss: 0.6493 - binary_accuracy: 0.6139 - val_loss: 0.6360 - val_binary_accuracy: 0.6529\n",
            "Epoch 3/4\n",
            "96/96 [==============================] - 52s 542ms/step - loss: 0.5546 - binary_accuracy: 0.7123 - val_loss: 0.6444 - val_binary_accuracy: 0.6706\n",
            "Epoch 4/4\n",
            "96/96 [==============================] - 52s 543ms/step - loss: 0.4581 - binary_accuracy: 0.7993 - val_loss: 0.6637 - val_binary_accuracy: 0.6735\n",
            "11/11 [==============================] - 2s 186ms/step - loss: 0.6637 - binary_accuracy: 0.6735\n",
            "Average error  0.6485294103622437\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Q1NGqLi0aGA",
        "outputId": "6629ce31-37f7-43fd-9275-b788d4aec7d0"
      },
      "source": [
        "# bert baseline, fine tuning\n",
        "bert_model_name = 'bert_en_uncased_L-12_H-768_A-12'\n",
        "epochs = 4\n",
        "init_lr = 3e-5\n",
        "batch_size = 16\n",
        "do_fine_tuning = True\n",
        "folds = 2\n",
        "\n",
        "def create_layers(net, text_input): \n",
        "  net = tf.keras.layers.Dropout(0.1)(net)\n",
        "  net = tf.keras.layers.Dense(1, activation='sigmoid', name='classifier')(net)\n",
        "  return tf.keras.Model(text_input, net)\n",
        "\n",
        "results, history, classifier_model = bert_10_fold(epochs=epochs, \n",
        "                                                  init_lr=init_lr, \n",
        "                                                  batch_size = batch_size, \n",
        "                                                  do_fine_tuning = do_fine_tuning, \n",
        "                                                  create_classifier = create_layers, \n",
        "                                                  bert_model_name = bert_model_name, \n",
        "                                                  folds = folds)\n",
        "\n",
        "print('Average error ', np.mean(results[bert_model_name]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bert_en_uncased_L-12_H-768_A-12\n",
            "FOLD  0\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "192/192 [==============================] - 71s 290ms/step - loss: 0.7091 - binary_accuracy: 0.4935 - val_loss: 0.6783 - val_binary_accuracy: 0.5765\n",
            "Epoch 2/4\n",
            "192/192 [==============================] - 55s 286ms/step - loss: 0.6449 - binary_accuracy: 0.6073 - val_loss: 0.6948 - val_binary_accuracy: 0.6118\n",
            "Epoch 3/4\n",
            "192/192 [==============================] - 55s 286ms/step - loss: 0.4853 - binary_accuracy: 0.7565 - val_loss: 0.8270 - val_binary_accuracy: 0.6206\n",
            "Epoch 4/4\n",
            "192/192 [==============================] - 55s 285ms/step - loss: 0.2863 - binary_accuracy: 0.8813 - val_loss: 0.9968 - val_binary_accuracy: 0.6353\n",
            "22/22 [==============================] - 2s 91ms/step - loss: 0.9968 - binary_accuracy: 0.6353\n",
            "FOLD  1\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "192/192 [==============================] - 71s 290ms/step - loss: 0.7075 - binary_accuracy: 0.5156 - val_loss: 0.6409 - val_binary_accuracy: 0.6147\n",
            "Epoch 2/4\n",
            "192/192 [==============================] - 55s 286ms/step - loss: 0.6324 - binary_accuracy: 0.6360 - val_loss: 0.6272 - val_binary_accuracy: 0.6588\n",
            "Epoch 3/4\n",
            "192/192 [==============================] - 55s 285ms/step - loss: 0.4621 - binary_accuracy: 0.7896 - val_loss: 0.8130 - val_binary_accuracy: 0.6500\n",
            "Epoch 4/4\n",
            "192/192 [==============================] - 55s 286ms/step - loss: 0.2963 - binary_accuracy: 0.8858 - val_loss: 1.0149 - val_binary_accuracy: 0.6471\n",
            "22/22 [==============================] - 2s 91ms/step - loss: 1.0149 - binary_accuracy: 0.6471\n",
            "Average error  0.6411764919757843\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndf429Kr2xKv",
        "outputId": "1589332f-4060-443e-a4aa-7bacd3f1dd02"
      },
      "source": [
        "# bert baseline, fine tuning\n",
        "bert_model_name = 'bert_en_uncased_L-12_H-768_A-12'\n",
        "epochs = 4\n",
        "init_lr = 3e-5\n",
        "batch_size = 32\n",
        "do_fine_tuning = True\n",
        "folds = 2\n",
        "\n",
        "def create_layers(net, text_input): \n",
        "  net = tf.keras.layers.Dropout(0.2)(net)\n",
        "  net = tf.keras.layers.Dense(1, activation='sigmoid', name='classifier')(net)\n",
        "  return tf.keras.Model(text_input, net)\n",
        "\n",
        "results, history, classifier_model = bert_10_fold(epochs=epochs, \n",
        "                                                  init_lr=init_lr, \n",
        "                                                  batch_size = batch_size, \n",
        "                                                  do_fine_tuning = do_fine_tuning, \n",
        "                                                  create_classifier = create_layers, \n",
        "                                                  bert_model_name = bert_model_name, \n",
        "                                                  folds = folds)\n",
        "\n",
        "print('Average error ', np.mean(results[bert_model_name]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bert_en_uncased_L-12_H-768_A-12\n",
            "FOLD  0\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "96/96 [==============================] - 67s 550ms/step - loss: 0.7391 - binary_accuracy: 0.4766 - val_loss: 0.6722 - val_binary_accuracy: 0.5971\n",
            "Epoch 2/4\n",
            "96/96 [==============================] - 52s 543ms/step - loss: 0.6542 - binary_accuracy: 0.6065 - val_loss: 0.6904 - val_binary_accuracy: 0.6235\n",
            "Epoch 3/4\n",
            "96/96 [==============================] - 52s 542ms/step - loss: 0.5462 - binary_accuracy: 0.7334 - val_loss: 0.7331 - val_binary_accuracy: 0.6118\n",
            "Epoch 4/4\n",
            "96/96 [==============================] - 52s 542ms/step - loss: 0.4221 - binary_accuracy: 0.8144 - val_loss: 0.8005 - val_binary_accuracy: 0.6147\n",
            "11/11 [==============================] - 2s 187ms/step - loss: 0.8005 - binary_accuracy: 0.6147\n",
            "FOLD  1\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "96/96 [==============================] - 66s 549ms/step - loss: 0.7558 - binary_accuracy: 0.5229 - val_loss: 0.6481 - val_binary_accuracy: 0.6265\n",
            "Epoch 2/4\n",
            "96/96 [==============================] - 52s 542ms/step - loss: 0.6532 - binary_accuracy: 0.6190 - val_loss: 0.6659 - val_binary_accuracy: 0.6441\n",
            "Epoch 3/4\n",
            "96/96 [==============================] - 52s 542ms/step - loss: 0.5294 - binary_accuracy: 0.7355 - val_loss: 0.6943 - val_binary_accuracy: 0.6441\n",
            "Epoch 4/4\n",
            "96/96 [==============================] - 52s 543ms/step - loss: 0.4252 - binary_accuracy: 0.8075 - val_loss: 0.7046 - val_binary_accuracy: 0.6412\n",
            "11/11 [==============================] - 2s 187ms/step - loss: 0.7046 - binary_accuracy: 0.6412\n",
            "Average error  0.6279411613941193\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ySI-3jeJhVs",
        "outputId": "889d8bd2-3dc6-49b1-fd6c-b631e57130de"
      },
      "source": [
        "# electra with fine-tuning\n",
        "bert_model_name = 'electra_base'\n",
        "epochs = 4\n",
        "init_lr = 2e-5\n",
        "batch_size = 32\n",
        "do_fine_tuning = True\n",
        "folds = 2\n",
        "\n",
        "def create_layers(net, text_input): \n",
        "  net = tf.keras.layers.Dropout(0.1)(net)\n",
        "  net = tf.keras.layers.Dense(1, activation='sigmoid', name='classifier')(net)\n",
        "  return tf.keras.Model(text_input, net)\n",
        "\n",
        "results, history, classifier_model = bert_10_fold(epochs=epochs, \n",
        "                                                  init_lr=init_lr, \n",
        "                                                  batch_size = batch_size, \n",
        "                                                  do_fine_tuning = do_fine_tuning, \n",
        "                                                  create_classifier = create_layers, \n",
        "                                                  bert_model_name = bert_model_name, \n",
        "                                                  folds = folds)\n",
        "\n",
        "print('Average error ', np.mean(results[bert_model_name]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "electra_base\n",
            "FOLD  0\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "96/96 [==============================] - 67s 551ms/step - loss: 0.7223 - binary_accuracy: 0.4873 - val_loss: 0.6723 - val_binary_accuracy: 0.5824\n",
            "Epoch 2/4\n",
            "96/96 [==============================] - 52s 543ms/step - loss: 0.6496 - binary_accuracy: 0.6185 - val_loss: 0.6700 - val_binary_accuracy: 0.6324\n",
            "Epoch 3/4\n",
            "96/96 [==============================] - 52s 543ms/step - loss: 0.5351 - binary_accuracy: 0.7265 - val_loss: 0.7188 - val_binary_accuracy: 0.6176\n",
            "Epoch 4/4\n",
            "96/96 [==============================] - 52s 543ms/step - loss: 0.4526 - binary_accuracy: 0.8029 - val_loss: 0.7171 - val_binary_accuracy: 0.6206\n",
            "11/11 [==============================] - 2s 186ms/step - loss: 0.7171 - binary_accuracy: 0.6206\n",
            "FOLD  1\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "96/96 [==============================] - 67s 550ms/step - loss: 0.7191 - binary_accuracy: 0.5058 - val_loss: 0.6555 - val_binary_accuracy: 0.5971\n",
            "Epoch 2/4\n",
            "96/96 [==============================] - 52s 543ms/step - loss: 0.6509 - binary_accuracy: 0.6149 - val_loss: 0.6251 - val_binary_accuracy: 0.6441\n",
            "Epoch 3/4\n",
            "96/96 [==============================] - 52s 543ms/step - loss: 0.5908 - binary_accuracy: 0.6907 - val_loss: 0.6134 - val_binary_accuracy: 0.6824\n",
            "Epoch 4/4\n",
            "96/96 [==============================] - 52s 542ms/step - loss: 0.5114 - binary_accuracy: 0.7560 - val_loss: 0.6254 - val_binary_accuracy: 0.6647\n",
            "11/11 [==============================] - 2s 187ms/step - loss: 0.6254 - binary_accuracy: 0.6647\n",
            "Average error  0.6426470577716827\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZmr4GLT7wDd",
        "outputId": "b6f032a7-fa88-4cf9-8157-8700090b47ef"
      },
      "source": [
        "# electra with fine-tuning\n",
        "bert_model_name = 'electra_base'\n",
        "epochs = 4\n",
        "init_lr = 2e-5\n",
        "batch_size = 16\n",
        "do_fine_tuning = True\n",
        "folds = 2\n",
        "\n",
        "def create_layers(net, text_input): \n",
        "  net = tf.keras.layers.Dropout(0.1)(net)\n",
        "  net = tf.keras.layers.Dense(1, activation='sigmoid', name='classifier')(net)\n",
        "  return tf.keras.Model(text_input, net)\n",
        "\n",
        "results, history, classifier_model = bert_10_fold(epochs=epochs, \n",
        "                                                  init_lr=init_lr, \n",
        "                                                  batch_size = batch_size, \n",
        "                                                  do_fine_tuning = do_fine_tuning, \n",
        "                                                  create_classifier = create_layers, \n",
        "                                                  bert_model_name = bert_model_name, \n",
        "                                                  folds = folds)\n",
        "\n",
        "print('Average error ', np.mean(results[bert_model_name]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "electra_base\n",
            "FOLD  0\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "192/192 [==============================] - 74s 290ms/step - loss: 0.7102 - binary_accuracy: 0.5062 - val_loss: 0.6992 - val_binary_accuracy: 0.5412\n",
            "Epoch 2/4\n",
            "192/192 [==============================] - 55s 286ms/step - loss: 0.6522 - binary_accuracy: 0.6113 - val_loss: 0.7016 - val_binary_accuracy: 0.6147\n",
            "Epoch 3/4\n",
            "192/192 [==============================] - 55s 286ms/step - loss: 0.4921 - binary_accuracy: 0.7597 - val_loss: 0.8453 - val_binary_accuracy: 0.6147\n",
            "Epoch 4/4\n",
            "192/192 [==============================] - 55s 286ms/step - loss: 0.3471 - binary_accuracy: 0.8588 - val_loss: 0.8401 - val_binary_accuracy: 0.6176\n",
            "22/22 [==============================] - 2s 92ms/step - loss: 0.8401 - binary_accuracy: 0.6176\n",
            "FOLD  1\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "192/192 [==============================] - 69s 289ms/step - loss: 0.7031 - binary_accuracy: 0.5168 - val_loss: 0.6521 - val_binary_accuracy: 0.5912\n",
            "Epoch 2/4\n",
            "192/192 [==============================] - 55s 286ms/step - loss: 0.6426 - binary_accuracy: 0.6234 - val_loss: 0.6118 - val_binary_accuracy: 0.6824\n",
            "Epoch 3/4\n",
            "192/192 [==============================] - 55s 286ms/step - loss: 0.5018 - binary_accuracy: 0.7563 - val_loss: 0.6562 - val_binary_accuracy: 0.6882\n",
            "Epoch 4/4\n",
            "192/192 [==============================] - 55s 286ms/step - loss: 0.3671 - binary_accuracy: 0.8413 - val_loss: 0.7002 - val_binary_accuracy: 0.6971\n",
            "22/22 [==============================] - 2s 92ms/step - loss: 0.7002 - binary_accuracy: 0.6971\n",
            "Average error  0.6573529243469238\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tj9OiKkK92pS",
        "outputId": "80cd36c4-ad9a-4fda-c360-ac7fe1a78257"
      },
      "source": [
        "# electra with fine-tuning\n",
        "bert_model_name = 'electra_base'\n",
        "epochs = 4\n",
        "init_lr = 2e-5\n",
        "batch_size = 8\n",
        "do_fine_tuning = True\n",
        "folds = 2\n",
        "\n",
        "def create_layers(net, text_input): \n",
        "  net = tf.keras.layers.Dropout(0.1)(net)\n",
        "  net = tf.keras.layers.Dense(1, activation='sigmoid', name='classifier')(net)\n",
        "  return tf.keras.Model(text_input, net)\n",
        "\n",
        "results, history, classifier_model = bert_10_fold(epochs=epochs, \n",
        "                                                  init_lr=init_lr, \n",
        "                                                  batch_size = batch_size, \n",
        "                                                  do_fine_tuning = do_fine_tuning, \n",
        "                                                  create_classifier = create_layers, \n",
        "                                                  bert_model_name = bert_model_name, \n",
        "                                                  folds = folds)\n",
        "\n",
        "print('Average error ', np.mean(results[bert_model_name]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "electra_base\n",
            "FOLD  0\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "383/383 [==============================] - 78s 165ms/step - loss: 0.7057 - binary_accuracy: 0.5254 - val_loss: 0.6516 - val_binary_accuracy: 0.6206\n",
            "Epoch 2/4\n",
            "383/383 [==============================] - 62s 163ms/step - loss: 0.6148 - binary_accuracy: 0.6757 - val_loss: 0.6861 - val_binary_accuracy: 0.6500\n",
            "Epoch 3/4\n",
            "383/383 [==============================] - 63s 163ms/step - loss: 0.4198 - binary_accuracy: 0.8272 - val_loss: 0.8328 - val_binary_accuracy: 0.6853\n",
            "Epoch 4/4\n",
            "383/383 [==============================] - 62s 163ms/step - loss: 0.2674 - binary_accuracy: 0.8968 - val_loss: 1.0019 - val_binary_accuracy: 0.6618\n",
            "43/43 [==============================] - 2s 49ms/step - loss: 1.0019 - binary_accuracy: 0.6618\n",
            "FOLD  1\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "383/383 [==============================] - 79s 165ms/step - loss: 0.7088 - binary_accuracy: 0.5200 - val_loss: 0.6436 - val_binary_accuracy: 0.6235\n",
            "Epoch 2/4\n",
            "383/383 [==============================] - 62s 163ms/step - loss: 0.6164 - binary_accuracy: 0.6678 - val_loss: 0.6605 - val_binary_accuracy: 0.6618\n",
            "Epoch 3/4\n",
            "383/383 [==============================] - 62s 163ms/step - loss: 0.4145 - binary_accuracy: 0.8280 - val_loss: 0.7751 - val_binary_accuracy: 0.7088\n",
            "Epoch 4/4\n",
            "383/383 [==============================] - 62s 163ms/step - loss: 0.2490 - binary_accuracy: 0.9034 - val_loss: 0.9428 - val_binary_accuracy: 0.7000\n",
            "43/43 [==============================] - 2s 49ms/step - loss: 0.9428 - binary_accuracy: 0.7000\n",
            "Average error  0.6808823347091675\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPOYPjO_ActA",
        "outputId": "f228c72b-d1cb-4bcc-fe0d-ce2b39531aac"
      },
      "source": [
        "# electra with fine-tuning\n",
        "bert_model_name = 'electra_base'\n",
        "epochs = 4\n",
        "init_lr = 2e-5\n",
        "batch_size = 8\n",
        "do_fine_tuning = True\n",
        "folds = 10\n",
        "\n",
        "def create_layers(net, text_input): \n",
        "  net = tf.keras.layers.Dropout(0.1)(net)\n",
        "  net = tf.keras.layers.Dense(1, activation='sigmoid', name='classifier')(net)\n",
        "  return tf.keras.Model(text_input, net)\n",
        "\n",
        "results, history, classifier_model = bert_10_fold(epochs=epochs, \n",
        "                                                  init_lr=init_lr, \n",
        "                                                  batch_size = batch_size, \n",
        "                                                  do_fine_tuning = do_fine_tuning, \n",
        "                                                  create_classifier = create_layers, \n",
        "                                                  bert_model_name = bert_model_name, \n",
        "                                                  folds = folds)\n",
        "\n",
        "print('Average error ', np.mean(results[bert_model_name]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "electra_base\n",
            "FOLD  0\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "383/383 [==============================] - 78s 165ms/step - loss: 0.7086 - binary_accuracy: 0.5139 - val_loss: 0.6642 - val_binary_accuracy: 0.5912\n",
            "Epoch 2/4\n",
            "383/383 [==============================] - 62s 162ms/step - loss: 0.6008 - binary_accuracy: 0.6650 - val_loss: 0.7506 - val_binary_accuracy: 0.6353\n",
            "Epoch 3/4\n",
            "383/383 [==============================] - 62s 163ms/step - loss: 0.4043 - binary_accuracy: 0.8273 - val_loss: 1.0335 - val_binary_accuracy: 0.6471\n",
            "Epoch 4/4\n",
            "383/383 [==============================] - 62s 163ms/step - loss: 0.2620 - binary_accuracy: 0.9054 - val_loss: 1.1740 - val_binary_accuracy: 0.6500\n",
            "43/43 [==============================] - 2s 50ms/step - loss: 1.1740 - binary_accuracy: 0.6500\n",
            "FOLD  1\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "383/383 [==============================] - 77s 164ms/step - loss: 0.7081 - binary_accuracy: 0.5246 - val_loss: 0.6329 - val_binary_accuracy: 0.6676\n",
            "Epoch 2/4\n",
            "383/383 [==============================] - 62s 163ms/step - loss: 0.6200 - binary_accuracy: 0.6552 - val_loss: 0.7080 - val_binary_accuracy: 0.6912\n",
            "Epoch 3/4\n",
            "383/383 [==============================] - 62s 163ms/step - loss: 0.4278 - binary_accuracy: 0.8119 - val_loss: 0.8339 - val_binary_accuracy: 0.7059\n",
            "Epoch 4/4\n",
            "383/383 [==============================] - 62s 163ms/step - loss: 0.2461 - binary_accuracy: 0.9059 - val_loss: 0.9735 - val_binary_accuracy: 0.6853\n",
            "43/43 [==============================] - 2s 50ms/step - loss: 0.9735 - binary_accuracy: 0.6853\n",
            "FOLD  2\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "383/383 [==============================] - 78s 164ms/step - loss: 0.6990 - binary_accuracy: 0.5238 - val_loss: 0.6273 - val_binary_accuracy: 0.6706\n",
            "Epoch 2/4\n",
            "383/383 [==============================] - 62s 163ms/step - loss: 0.6121 - binary_accuracy: 0.6542 - val_loss: 0.6669 - val_binary_accuracy: 0.6559\n",
            "Epoch 3/4\n",
            "383/383 [==============================] - 62s 162ms/step - loss: 0.3999 - binary_accuracy: 0.8173 - val_loss: 0.8035 - val_binary_accuracy: 0.6500\n",
            "Epoch 4/4\n",
            "383/383 [==============================] - 62s 162ms/step - loss: 0.2219 - binary_accuracy: 0.9159 - val_loss: 1.0802 - val_binary_accuracy: 0.6647\n",
            "43/43 [==============================] - 2s 49ms/step - loss: 1.0802 - binary_accuracy: 0.6647\n",
            "FOLD  3\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "383/383 [==============================] - 77s 164ms/step - loss: 0.7040 - binary_accuracy: 0.5009 - val_loss: 0.6691 - val_binary_accuracy: 0.5941\n",
            "Epoch 2/4\n",
            "383/383 [==============================] - 62s 163ms/step - loss: 0.6111 - binary_accuracy: 0.6604 - val_loss: 0.7006 - val_binary_accuracy: 0.6618\n",
            "Epoch 3/4\n",
            "383/383 [==============================] - 62s 162ms/step - loss: 0.4446 - binary_accuracy: 0.7897 - val_loss: 0.9054 - val_binary_accuracy: 0.6676\n",
            "Epoch 4/4\n",
            "383/383 [==============================] - 62s 163ms/step - loss: 0.3079 - binary_accuracy: 0.8789 - val_loss: 1.0062 - val_binary_accuracy: 0.6824\n",
            "43/43 [==============================] - 2s 49ms/step - loss: 1.0062 - binary_accuracy: 0.6824\n",
            "FOLD  4\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "383/383 [==============================] - 78s 165ms/step - loss: 0.7105 - binary_accuracy: 0.5082 - val_loss: 0.6554 - val_binary_accuracy: 0.6265\n",
            "Epoch 2/4\n",
            "383/383 [==============================] - 62s 163ms/step - loss: 0.6064 - binary_accuracy: 0.6790 - val_loss: 0.7699 - val_binary_accuracy: 0.6353\n",
            "Epoch 3/4\n",
            "383/383 [==============================] - 62s 162ms/step - loss: 0.4233 - binary_accuracy: 0.8168 - val_loss: 1.0356 - val_binary_accuracy: 0.6500\n",
            "Epoch 4/4\n",
            "383/383 [==============================] - 62s 162ms/step - loss: 0.2664 - binary_accuracy: 0.8983 - val_loss: 1.2678 - val_binary_accuracy: 0.6588\n",
            "43/43 [==============================] - 2s 49ms/step - loss: 1.2678 - binary_accuracy: 0.6588\n",
            "FOLD  5\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "383/383 [==============================] - 80s 165ms/step - loss: 0.6954 - binary_accuracy: 0.5374 - val_loss: 0.6764 - val_binary_accuracy: 0.6294\n",
            "Epoch 2/4\n",
            "383/383 [==============================] - 62s 163ms/step - loss: 0.6025 - binary_accuracy: 0.6647 - val_loss: 0.7331 - val_binary_accuracy: 0.6324\n",
            "Epoch 3/4\n",
            "383/383 [==============================] - 62s 162ms/step - loss: 0.4691 - binary_accuracy: 0.7767 - val_loss: 0.9020 - val_binary_accuracy: 0.6412\n",
            "Epoch 4/4\n",
            "383/383 [==============================] - 62s 163ms/step - loss: 0.3085 - binary_accuracy: 0.8747 - val_loss: 1.0677 - val_binary_accuracy: 0.6294\n",
            "43/43 [==============================] - 2s 50ms/step - loss: 1.0677 - binary_accuracy: 0.6294\n",
            "FOLD  6\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "383/383 [==============================] - 77s 165ms/step - loss: 0.6923 - binary_accuracy: 0.5578 - val_loss: 0.6370 - val_binary_accuracy: 0.6353\n",
            "Epoch 2/4\n",
            "383/383 [==============================] - 62s 162ms/step - loss: 0.6136 - binary_accuracy: 0.6596 - val_loss: 0.6941 - val_binary_accuracy: 0.6471\n",
            "Epoch 3/4\n",
            "383/383 [==============================] - 62s 163ms/step - loss: 0.4159 - binary_accuracy: 0.8194 - val_loss: 0.7226 - val_binary_accuracy: 0.6706\n",
            "Epoch 4/4\n",
            "383/383 [==============================] - 63s 163ms/step - loss: 0.2351 - binary_accuracy: 0.9022 - val_loss: 0.9984 - val_binary_accuracy: 0.6676\n",
            "43/43 [==============================] - 2s 52ms/step - loss: 0.9984 - binary_accuracy: 0.6676\n",
            "FOLD  7\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "383/383 [==============================] - 80s 166ms/step - loss: 0.7120 - binary_accuracy: 0.5134 - val_loss: 0.6343 - val_binary_accuracy: 0.6294\n",
            "Epoch 2/4\n",
            "383/383 [==============================] - 63s 165ms/step - loss: 0.5991 - binary_accuracy: 0.6719 - val_loss: 0.6159 - val_binary_accuracy: 0.6853\n",
            "Epoch 3/4\n",
            "383/383 [==============================] - 63s 165ms/step - loss: 0.4143 - binary_accuracy: 0.8146 - val_loss: 0.8668 - val_binary_accuracy: 0.6559\n",
            "Epoch 4/4\n",
            "383/383 [==============================] - 63s 165ms/step - loss: 0.2307 - binary_accuracy: 0.9009 - val_loss: 1.2097 - val_binary_accuracy: 0.6471\n",
            "43/43 [==============================] - 2s 52ms/step - loss: 1.2097 - binary_accuracy: 0.6471\n",
            "FOLD  8\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "383/383 [==============================] - 83s 167ms/step - loss: 0.7146 - binary_accuracy: 0.4982 - val_loss: 0.6439 - val_binary_accuracy: 0.6324\n",
            "Epoch 2/4\n",
            "383/383 [==============================] - 63s 165ms/step - loss: 0.6060 - binary_accuracy: 0.6685 - val_loss: 0.7973 - val_binary_accuracy: 0.6118\n",
            "Epoch 3/4\n",
            "383/383 [==============================] - 63s 165ms/step - loss: 0.4187 - binary_accuracy: 0.8123 - val_loss: 1.0211 - val_binary_accuracy: 0.6441\n",
            "Epoch 4/4\n",
            "383/383 [==============================] - 63s 165ms/step - loss: 0.2782 - binary_accuracy: 0.8910 - val_loss: 1.1132 - val_binary_accuracy: 0.6765\n",
            "43/43 [==============================] - 2s 52ms/step - loss: 1.1132 - binary_accuracy: 0.6765\n",
            "FOLD  9\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "383/383 [==============================] - 80s 167ms/step - loss: 0.7125 - binary_accuracy: 0.5173 - val_loss: 0.6424 - val_binary_accuracy: 0.6294\n",
            "Epoch 2/4\n",
            "383/383 [==============================] - 63s 164ms/step - loss: 0.6040 - binary_accuracy: 0.6729 - val_loss: 0.7064 - val_binary_accuracy: 0.6471\n",
            "Epoch 3/4\n",
            "383/383 [==============================] - 63s 165ms/step - loss: 0.3997 - binary_accuracy: 0.8156 - val_loss: 0.9631 - val_binary_accuracy: 0.6324\n",
            "Epoch 4/4\n",
            "383/383 [==============================] - 63s 164ms/step - loss: 0.2375 - binary_accuracy: 0.9036 - val_loss: 1.1594 - val_binary_accuracy: 0.6529\n",
            "43/43 [==============================] - 2s 50ms/step - loss: 1.1594 - binary_accuracy: 0.6529\n",
            "Average error  0.6614705860614777\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTdeFYZMANG_"
      },
      "source": [
        "# electra with fine-tuning\n",
        "bert_model_name = 'electra_base'\n",
        "epochs = 4\n",
        "init_lr = 2e-5\n",
        "batch_size = 8\n",
        "do_fine_tuning = True\n",
        "folds = 2\n",
        "\n",
        "def create_layers(net, text_input): \n",
        "  net = tf.keras.layers.Dropout(0.2)(net)\n",
        "  net = tf.keras.layers.Dense(1, activation='sigmoid', name='classifier')(net)\n",
        "  return tf.keras.Model(text_input, net)\n",
        "\n",
        "results, history, classifier_model = bert_10_fold(epochs=epochs, \n",
        "                                                  init_lr=init_lr, \n",
        "                                                  batch_size = batch_size, \n",
        "                                                  do_fine_tuning = do_fine_tuning, \n",
        "                                                  create_classifier = create_layers, \n",
        "                                                  bert_model_name = bert_model_name, \n",
        "                                                  folds = folds)\n",
        "\n",
        "print('Average error ', np.mean(results[bert_model_name]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvbDjrJRCo8d",
        "outputId": "d0da3d39-2c53-4df0-c50e-3d8e92b1bbf9"
      },
      "source": [
        "# electra with fine-tuning, regularization\n",
        "bert_model_name = 'electra_base'\n",
        "epochs = 4\n",
        "init_lr = 2e-5\n",
        "batch_size = 8\n",
        "do_fine_tuning = True\n",
        "folds = 2\n",
        "\n",
        "def create_layers(net, text_input): \n",
        "  net = tf.keras.layers.Dropout(0.2)(net)\n",
        "  net = tf.keras.layers.Dense(1, activation='sigmoid', name='classifier', \n",
        "                              kernel_regularizer=tf.keras.regularizers.l2(0.01)\n",
        "                              )(net)\n",
        "  return tf.keras.Model(text_input, net)\n",
        "#tf.keras.regularizers.L1L2(l1=0.01, l2=0.01)\n",
        "results, history, classifier_model = bert_10_fold(epochs=epochs, \n",
        "                                                  init_lr=init_lr, \n",
        "                                                  batch_size = batch_size, \n",
        "                                                  do_fine_tuning = do_fine_tuning, \n",
        "                                                  create_classifier = create_layers, \n",
        "                                                  bert_model_name = bert_model_name, \n",
        "                                                  folds = folds)\n",
        "\n",
        "print('Average error ', np.mean(results[bert_model_name]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "electra_base\n",
            "FOLD  0\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "383/383 [==============================] - 80s 165ms/step - loss: 0.7439 - binary_accuracy: 0.4972 - val_loss: 0.6977 - val_binary_accuracy: 0.6029\n",
            "Epoch 2/4\n",
            "383/383 [==============================] - 63s 164ms/step - loss: 0.6288 - binary_accuracy: 0.6692 - val_loss: 0.6961 - val_binary_accuracy: 0.6647\n",
            "Epoch 3/4\n",
            "383/383 [==============================] - 63s 164ms/step - loss: 0.4410 - binary_accuracy: 0.8128 - val_loss: 0.9576 - val_binary_accuracy: 0.6294\n",
            "Epoch 4/4\n",
            "383/383 [==============================] - 63s 164ms/step - loss: 0.2615 - binary_accuracy: 0.9101 - val_loss: 1.3305 - val_binary_accuracy: 0.6118\n",
            "43/43 [==============================] - 2s 51ms/step - loss: 1.3305 - binary_accuracy: 0.6118\n",
            "FOLD  1\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "383/383 [==============================] - 77s 165ms/step - loss: 0.7325 - binary_accuracy: 0.4937 - val_loss: 0.6556 - val_binary_accuracy: 0.6500\n",
            "Epoch 2/4\n",
            "383/383 [==============================] - 63s 164ms/step - loss: 0.6493 - binary_accuracy: 0.6494 - val_loss: 0.6374 - val_binary_accuracy: 0.6824\n",
            "Epoch 3/4\n",
            "383/383 [==============================] - 63s 163ms/step - loss: 0.4596 - binary_accuracy: 0.7944 - val_loss: 0.8092 - val_binary_accuracy: 0.6912\n",
            "Epoch 4/4\n",
            "383/383 [==============================] - 63s 164ms/step - loss: 0.2960 - binary_accuracy: 0.8904 - val_loss: 0.8995 - val_binary_accuracy: 0.7000\n",
            "43/43 [==============================] - 2s 50ms/step - loss: 0.8995 - binary_accuracy: 0.7000\n",
            "Average error  0.6558823585510254\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wn0XKlqyE85n",
        "outputId": "24158eb5-b0e0-43e5-83db-b71c185b6340"
      },
      "source": [
        "# electra with fine-tuning\n",
        "bert_model_name = 'electra_base'\n",
        "epochs = 4\n",
        "init_lr = 2e-5\n",
        "batch_size = 8\n",
        "do_fine_tuning = True\n",
        "folds = 2\n",
        "\n",
        "def create_layers(net, text_input): \n",
        "  net = tf.keras.layers.Dropout(0.1)(net)\n",
        "  net = tf.keras.layers.Dense(1, activation='sigmoid', name='classifier', \n",
        "                              kernel_regularizer=tf.keras.regularizers.L1L2(l1=0.01, l2=0.01)\n",
        "                              )(net)\n",
        "  return tf.keras.Model(text_input, net)\n",
        "\n",
        "results, history, classifier_model = bert_10_fold(epochs=epochs, \n",
        "                                                  init_lr=init_lr, \n",
        "                                                  batch_size = batch_size, \n",
        "                                                  do_fine_tuning = do_fine_tuning, \n",
        "                                                  create_classifier = create_layers, \n",
        "                                                  bert_model_name = bert_model_name, \n",
        "                                                  folds = folds)\n",
        "\n",
        "print('Average error ', np.mean(results[bert_model_name]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "electra_base\n",
            "FOLD  0\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "383/383 [==============================] - 80s 165ms/step - loss: 1.0369 - binary_accuracy: 0.5390 - val_loss: 0.9898 - val_binary_accuracy: 0.6118\n",
            "Epoch 2/4\n",
            "383/383 [==============================] - 63s 163ms/step - loss: 0.9364 - binary_accuracy: 0.6748 - val_loss: 1.0130 - val_binary_accuracy: 0.6588\n",
            "Epoch 3/4\n",
            "383/383 [==============================] - 63s 163ms/step - loss: 0.7527 - binary_accuracy: 0.8117 - val_loss: 1.2511 - val_binary_accuracy: 0.6559\n",
            "Epoch 4/4\n",
            "383/383 [==============================] - 62s 163ms/step - loss: 0.5728 - binary_accuracy: 0.9103 - val_loss: 1.4447 - val_binary_accuracy: 0.6706\n",
            "43/43 [==============================] - 2s 50ms/step - loss: 1.4447 - binary_accuracy: 0.6706\n",
            "FOLD  1\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "383/383 [==============================] - 79s 165ms/step - loss: 1.0758 - binary_accuracy: 0.4985 - val_loss: 0.9979 - val_binary_accuracy: 0.6353\n",
            "Epoch 2/4\n",
            "383/383 [==============================] - 62s 163ms/step - loss: 0.9728 - binary_accuracy: 0.6563 - val_loss: 0.9663 - val_binary_accuracy: 0.6912\n",
            "Epoch 3/4\n",
            "383/383 [==============================] - 62s 163ms/step - loss: 0.7757 - binary_accuracy: 0.8035 - val_loss: 1.0450 - val_binary_accuracy: 0.7059\n",
            "Epoch 4/4\n",
            "383/383 [==============================] - 62s 163ms/step - loss: 0.5913 - binary_accuracy: 0.9007 - val_loss: 1.2429 - val_binary_accuracy: 0.7206\n",
            "43/43 [==============================] - 2s 50ms/step - loss: 1.2429 - binary_accuracy: 0.7206\n",
            "Average error  0.695588231086731\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RzwbT3NjGSmE",
        "outputId": "984ddbb6-8e29-475d-f3d3-c3dda297380f"
      },
      "source": [
        "# electra with fine-tuning, relu\n",
        "bert_model_name = 'electra_base'\n",
        "epochs = 4\n",
        "init_lr = 2e-5\n",
        "batch_size = 8\n",
        "do_fine_tuning = True\n",
        "folds = 2\n",
        "\n",
        "def create_layers(net, text_input): \n",
        "  net = tf.keras.layers.Dense(256, activation = 'relu')(net)\n",
        "  net = tf.keras.layers.Dropout(0.1)(net)\n",
        "  net = tf.keras.layers.Dense(1, activation='sigmoid', name='classifier', \n",
        "                              kernel_regularizer=tf.keras.regularizers.L1L2(l1=0.01, l2=0.01)\n",
        "                              )(net)\n",
        "  return tf.keras.Model(text_input, net)\n",
        "\n",
        "results, history, classifier_model = bert_10_fold(epochs=epochs, \n",
        "                                                  init_lr=init_lr, \n",
        "                                                  batch_size = batch_size, \n",
        "                                                  do_fine_tuning = do_fine_tuning, \n",
        "                                                  create_classifier = create_layers, \n",
        "                                                  bert_model_name = bert_model_name, \n",
        "                                                  folds = folds)\n",
        "\n",
        "print('Average error ', np.mean(results[bert_model_name]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "electra_base\n",
            "FOLD  0\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "383/383 [==============================] - 81s 166ms/step - loss: 0.9102 - binary_accuracy: 0.5336 - val_loss: 0.8693 - val_binary_accuracy: 0.6029\n",
            "Epoch 2/4\n",
            "383/383 [==============================] - 63s 164ms/step - loss: 0.8254 - binary_accuracy: 0.6699 - val_loss: 0.8735 - val_binary_accuracy: 0.6353\n",
            "Epoch 3/4\n",
            "383/383 [==============================] - 63s 164ms/step - loss: 0.6634 - binary_accuracy: 0.7973 - val_loss: 1.0584 - val_binary_accuracy: 0.6529\n",
            "Epoch 4/4\n",
            "383/383 [==============================] - 63s 164ms/step - loss: 0.4962 - binary_accuracy: 0.8868 - val_loss: 1.2075 - val_binary_accuracy: 0.6588\n",
            "43/43 [==============================] - 2s 51ms/step - loss: 1.2075 - binary_accuracy: 0.6588\n",
            "FOLD  1\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "383/383 [==============================] - 78s 166ms/step - loss: 0.9269 - binary_accuracy: 0.5269 - val_loss: 0.8690 - val_binary_accuracy: 0.6441\n",
            "Epoch 2/4\n",
            "383/383 [==============================] - 63s 164ms/step - loss: 0.8412 - binary_accuracy: 0.6426 - val_loss: 0.8831 - val_binary_accuracy: 0.6382\n",
            "Epoch 3/4\n",
            "383/383 [==============================] - 63s 164ms/step - loss: 0.6600 - binary_accuracy: 0.7768 - val_loss: 1.0314 - val_binary_accuracy: 0.6824\n",
            "Epoch 4/4\n",
            "383/383 [==============================] - 63s 164ms/step - loss: 0.4992 - binary_accuracy: 0.8860 - val_loss: 1.1709 - val_binary_accuracy: 0.6706\n",
            "43/43 [==============================] - 2s 51ms/step - loss: 1.1709 - binary_accuracy: 0.6706\n",
            "Average error  0.664705902338028\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qAR68XhIGOfG",
        "outputId": "282e6d93-06d1-4da4-c5bf-d800301fff96"
      },
      "source": [
        "# electra with fine-tuning, CNN \n",
        "bert_model_name = 'electra_base'\n",
        "epochs = 4\n",
        "init_lr = 2e-5\n",
        "batch_size = 8\n",
        "do_fine_tuning = True\n",
        "folds = 2\n",
        "\n",
        "def create_layers(net, text_input): \n",
        "  filter_length = 1\n",
        "  seq_len = 256\n",
        "  net = tf.keras.layers.Reshape((2,384), input_shape=(None,768))(net)\n",
        "  net = tf.keras.layers.Conv1D(filters=256, kernel_size=(filter_length), padding='valid',strides=1, activation='relu')(net)\n",
        "  net = tf.keras.layers.MaxPool1D(pool_size=2)(net)\n",
        "  net = tf.keras.layers.Flatten()(net)\n",
        "  net = tf.keras.layers.Dense(256, activation = 'relu')(net)\n",
        "  net = tf.keras.layers.Dropout(0.2)(net)\n",
        "  net = tf.keras.layers.Dense(1, activation='sigmoid', name='classifier', \n",
        "                              kernel_regularizer=tf.keras.regularizers.L1L2(l1=0.01, l2=0.01)\n",
        "                              )(net)\n",
        "  return tf.keras.Model(text_input, net)\n",
        "\n",
        "results, history, classifier_model = bert_10_fold(epochs=epochs, \n",
        "                                                  init_lr=init_lr, \n",
        "                                                  batch_size = batch_size, \n",
        "                                                  do_fine_tuning = do_fine_tuning, \n",
        "                                                  create_classifier = create_layers, \n",
        "                                                  bert_model_name = bert_model_name, \n",
        "                                                  folds = folds)\n",
        "\n",
        "print('Average error ', np.mean(results[bert_model_name]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "electra_base\n",
            "FOLD  0\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "383/383 [==============================] - 95s 166ms/step - loss: 0.9254 - binary_accuracy: 0.4971 - val_loss: 0.8844 - val_binary_accuracy: 0.5882\n",
            "Epoch 2/4\n",
            "383/383 [==============================] - 63s 164ms/step - loss: 0.8737 - binary_accuracy: 0.6121 - val_loss: 0.8727 - val_binary_accuracy: 0.6412\n",
            "Epoch 3/4\n",
            "383/383 [==============================] - 63s 164ms/step - loss: 0.7263 - binary_accuracy: 0.7464 - val_loss: 0.9747 - val_binary_accuracy: 0.6647\n",
            "Epoch 4/4\n",
            "383/383 [==============================] - 63s 164ms/step - loss: 0.5540 - binary_accuracy: 0.8625 - val_loss: 1.0818 - val_binary_accuracy: 0.6706\n",
            "43/43 [==============================] - 2s 50ms/step - loss: 1.0818 - binary_accuracy: 0.6706\n",
            "FOLD  1\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "383/383 [==============================] - 80s 166ms/step - loss: 0.9252 - binary_accuracy: 0.5075 - val_loss: 0.8735 - val_binary_accuracy: 0.6147\n",
            "Epoch 2/4\n",
            "383/383 [==============================] - 63s 164ms/step - loss: 0.8762 - binary_accuracy: 0.6100 - val_loss: 0.8178 - val_binary_accuracy: 0.6765\n",
            "Epoch 3/4\n",
            "383/383 [==============================] - 63s 164ms/step - loss: 0.7481 - binary_accuracy: 0.7369 - val_loss: 0.8751 - val_binary_accuracy: 0.6735\n",
            "Epoch 4/4\n",
            "383/383 [==============================] - 63s 164ms/step - loss: 0.5979 - binary_accuracy: 0.8391 - val_loss: 0.9670 - val_binary_accuracy: 0.6676\n",
            "43/43 [==============================] - 2s 51ms/step - loss: 0.9670 - binary_accuracy: 0.6676\n",
            "Average error  0.669117659330368\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9YsS4u_WSnK",
        "outputId": "1ddb63d6-8749-46d6-96dc-b9894078d483"
      },
      "source": [
        "# electra with fine-tuning, CNN \n",
        "bert_model_name = 'electra_base'\n",
        "epochs = 4\n",
        "init_lr = 2e-5\n",
        "batch_size = 8\n",
        "do_fine_tuning = True\n",
        "folds = 2\n",
        "\n",
        "def create_layers(net, text_input): \n",
        "  filter_length = 1\n",
        "  seq_len = 256\n",
        "  net = tf.keras.layers.Reshape((2,384), input_shape=(None,768))(net)\n",
        "  net = tf.keras.layers.Conv1D(filters=256, kernel_size=(filter_length), padding='valid',strides=1, activation='relu')(net)\n",
        "  net = tf.keras.layers.MaxPool1D(pool_size=2)(net)\n",
        "  net = tf.keras.layers.Flatten()(net)\n",
        "  net = tf.keras.layers.Dropout(0.2)(net)\n",
        "  net = tf.keras.layers.Dense(1, activation='sigmoid', name='classifier', \n",
        "                              kernel_regularizer=tf.keras.regularizers.L1L2(l1=0.01, l2=0.01)\n",
        "                              )(net)\n",
        "  return tf.keras.Model(text_input, net)\n",
        "\n",
        "results, history, classifier_model = bert_10_fold(epochs=epochs, \n",
        "                                                  init_lr=init_lr, \n",
        "                                                  batch_size = batch_size, \n",
        "                                                  do_fine_tuning = do_fine_tuning, \n",
        "                                                  create_classifier = create_layers, \n",
        "                                                  bert_model_name = bert_model_name, \n",
        "                                                  folds = folds)\n",
        "\n",
        "print('Average error ', np.mean(results[bert_model_name]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "electra_base\n",
            "FOLD  0\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "383/383 [==============================] - 81s 166ms/step - loss: 0.9034 - binary_accuracy: 0.5243 - val_loss: 0.8629 - val_binary_accuracy: 0.5735\n",
            "Epoch 2/4\n",
            "383/383 [==============================] - 63s 164ms/step - loss: 0.8404 - binary_accuracy: 0.6184 - val_loss: 0.8713 - val_binary_accuracy: 0.6324\n",
            "Epoch 3/4\n",
            "383/383 [==============================] - 63s 164ms/step - loss: 0.6666 - binary_accuracy: 0.7864 - val_loss: 1.1397 - val_binary_accuracy: 0.6147\n",
            "Epoch 4/4\n",
            "383/383 [==============================] - 63s 164ms/step - loss: 0.4878 - binary_accuracy: 0.8846 - val_loss: 1.2642 - val_binary_accuracy: 0.6265\n",
            "43/43 [==============================] - 2s 51ms/step - loss: 1.2642 - binary_accuracy: 0.6265\n",
            "FOLD  1\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "383/383 [==============================] - 80s 166ms/step - loss: 0.9202 - binary_accuracy: 0.5054 - val_loss: 0.8585 - val_binary_accuracy: 0.6235\n",
            "Epoch 2/4\n",
            "383/383 [==============================] - 63s 164ms/step - loss: 0.8467 - binary_accuracy: 0.6364 - val_loss: 0.8346 - val_binary_accuracy: 0.6941\n",
            "Epoch 3/4\n",
            "383/383 [==============================] - 63s 165ms/step - loss: 0.6950 - binary_accuracy: 0.7712 - val_loss: 0.9029 - val_binary_accuracy: 0.6971\n",
            "Epoch 4/4\n",
            "383/383 [==============================] - 63s 165ms/step - loss: 0.5168 - binary_accuracy: 0.8694 - val_loss: 1.0334 - val_binary_accuracy: 0.6912\n",
            "43/43 [==============================] - 2s 51ms/step - loss: 1.0334 - binary_accuracy: 0.6912\n",
            "Average error  0.6588235199451447\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZyMC3B-Xujq",
        "outputId": "59228910-b63f-46eb-9b49-8f60ce023e23"
      },
      "source": [
        "# electra with fine-tuning, CNN, remove max pool input\n",
        "bert_model_name = 'electra_base'\n",
        "epochs = 4\n",
        "init_lr = 2e-5\n",
        "batch_size = 8\n",
        "do_fine_tuning = True\n",
        "folds = 2\n",
        "\n",
        "def create_layers(net, text_input): \n",
        "  filter_length = 1\n",
        "  seq_len = 256\n",
        "  net = tf.keras.layers.Reshape((2,384), input_shape=(None,768))(net)\n",
        "  net = tf.keras.layers.Conv1D(filters=256, kernel_size=(filter_length), padding='valid',strides=1, activation='relu')(net)\n",
        "  net = tf.keras.layers.MaxPool1D()(net)\n",
        "  net = tf.keras.layers.Flatten()(net)\n",
        "  net = tf.keras.layers.Dense(256, activation = 'relu')(net)\n",
        "  net = tf.keras.layers.Dropout(0.2)(net)\n",
        "  net = tf.keras.layers.Dense(1, activation='sigmoid', name='classifier', \n",
        "                              kernel_regularizer=tf.keras.regularizers.L1L2(l1=0.01, l2=0.01)\n",
        "                              )(net)\n",
        "  return tf.keras.Model(text_input, net)\n",
        "\n",
        "results, history, classifier_model = bert_10_fold(epochs=epochs, \n",
        "                                                  init_lr=init_lr, \n",
        "                                                  batch_size = batch_size, \n",
        "                                                  do_fine_tuning = do_fine_tuning, \n",
        "                                                  create_classifier = create_layers, \n",
        "                                                  bert_model_name = bert_model_name, \n",
        "                                                  folds = folds)\n",
        "\n",
        "print('Average error ', np.mean(results[bert_model_name]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "electra_base\n",
            "FOLD  0\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "383/383 [==============================] - 79s 167ms/step - loss: 0.9292 - binary_accuracy: 0.5219 - val_loss: 0.8794 - val_binary_accuracy: 0.6118\n",
            "Epoch 2/4\n",
            "383/383 [==============================] - 63s 165ms/step - loss: 0.8658 - binary_accuracy: 0.6182 - val_loss: 0.8789 - val_binary_accuracy: 0.6265\n",
            "Epoch 3/4\n",
            "383/383 [==============================] - 63s 165ms/step - loss: 0.7371 - binary_accuracy: 0.7534 - val_loss: 1.0353 - val_binary_accuracy: 0.6529\n",
            "Epoch 4/4\n",
            "383/383 [==============================] - 63s 165ms/step - loss: 0.6161 - binary_accuracy: 0.8357 - val_loss: 1.0763 - val_binary_accuracy: 0.6412\n",
            "43/43 [==============================] - 2s 53ms/step - loss: 1.0763 - binary_accuracy: 0.6412\n",
            "FOLD  1\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "383/383 [==============================] - 79s 167ms/step - loss: 0.9121 - binary_accuracy: 0.5169 - val_loss: 0.8427 - val_binary_accuracy: 0.6735\n",
            "Epoch 2/4\n",
            "383/383 [==============================] - 63s 165ms/step - loss: 0.8476 - binary_accuracy: 0.6337 - val_loss: 0.7907 - val_binary_accuracy: 0.7000\n",
            "Epoch 3/4\n",
            "383/383 [==============================] - 63s 165ms/step - loss: 0.6970 - binary_accuracy: 0.7696 - val_loss: 0.8652 - val_binary_accuracy: 0.6882\n",
            "Epoch 4/4\n",
            "383/383 [==============================] - 63s 164ms/step - loss: 0.5724 - binary_accuracy: 0.8458 - val_loss: 0.9414 - val_binary_accuracy: 0.7029\n",
            "43/43 [==============================] - 2s 51ms/step - loss: 0.9414 - binary_accuracy: 0.7029\n",
            "Average error  0.6720588207244873\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxuqIhEQFBt9",
        "outputId": "838a5dde-db18-4640-e872-3905da3af975"
      },
      "source": [
        "# electra with fine-tuning, CNN \n",
        "bert_model_name = 'electra_base'\n",
        "epochs = 4\n",
        "init_lr = 2e-5\n",
        "batch_size = 8\n",
        "do_fine_tuning = True\n",
        "folds = 2\n",
        "\n",
        "def create_layers(net, text_input): \n",
        "  filter_length = 2\n",
        "  seq_len = 256\n",
        "  net = tf.keras.layers.Reshape((4,192), input_shape=(None,768))(net)\n",
        "  net = tf.keras.layers.Conv1D(filters=128, kernel_size=(filter_length), padding='valid',strides=1, activation='relu')(net)\n",
        "  net = tf.keras.layers.MaxPool1D(pool_size=2)(net)\n",
        "  net = tf.keras.layers.Flatten()(net)\n",
        "  net = tf.keras.layers.Dense(128, activation = 'tanh')(net)\n",
        "  net = tf.keras.layers.Dropout(0.2)(net)\n",
        "  net = tf.keras.layers.Dense(1, activation='sigmoid', name='classifier', \n",
        "                              kernel_regularizer=tf.keras.regularizers.L1L2(l1=0.01, l2=0.01)\n",
        "                              )(net)\n",
        "  return tf.keras.Model(text_input, net)\n",
        "\n",
        "results, history, classifier_model = bert_10_fold(epochs=epochs, \n",
        "                                                  init_lr=init_lr, \n",
        "                                                  batch_size = batch_size, \n",
        "                                                  do_fine_tuning = do_fine_tuning, \n",
        "                                                  create_classifier = create_layers, \n",
        "                                                  bert_model_name = bert_model_name, \n",
        "                                                  folds = folds)\n",
        "\n",
        "print('Average error ', np.mean(results[bert_model_name]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "electra_base\n",
            "FOLD  0\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "383/383 [==============================] - 82s 166ms/step - loss: 0.8582 - binary_accuracy: 0.4923 - val_loss: 0.8242 - val_binary_accuracy: 0.5882\n",
            "Epoch 2/4\n",
            "383/383 [==============================] - 63s 164ms/step - loss: 0.7852 - binary_accuracy: 0.6400 - val_loss: 0.8243 - val_binary_accuracy: 0.6176\n",
            "Epoch 3/4\n",
            "383/383 [==============================] - 63s 164ms/step - loss: 0.6319 - binary_accuracy: 0.7727 - val_loss: 1.0018 - val_binary_accuracy: 0.6353\n",
            "Epoch 4/4\n",
            "383/383 [==============================] - 63s 164ms/step - loss: 0.4787 - binary_accuracy: 0.8696 - val_loss: 1.0712 - val_binary_accuracy: 0.6471\n",
            "43/43 [==============================] - 2s 51ms/step - loss: 1.0712 - binary_accuracy: 0.6471\n",
            "FOLD  1\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "383/383 [==============================] - 78s 166ms/step - loss: 0.8568 - binary_accuracy: 0.5078 - val_loss: 0.7918 - val_binary_accuracy: 0.6324\n",
            "Epoch 2/4\n",
            "383/383 [==============================] - 63s 164ms/step - loss: 0.7960 - binary_accuracy: 0.6323 - val_loss: 0.7416 - val_binary_accuracy: 0.7029\n",
            "Epoch 3/4\n",
            "383/383 [==============================] - 63s 164ms/step - loss: 0.6364 - binary_accuracy: 0.7748 - val_loss: 0.7964 - val_binary_accuracy: 0.7029\n",
            "Epoch 4/4\n",
            "383/383 [==============================] - 63s 164ms/step - loss: 0.4578 - binary_accuracy: 0.8821 - val_loss: 0.8906 - val_binary_accuracy: 0.6912\n",
            "43/43 [==============================] - 2s 51ms/step - loss: 0.8906 - binary_accuracy: 0.6912\n",
            "Average error  0.669117659330368\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LekCmyRbuIwg",
        "outputId": "3949291e-884d-4cfe-d370-5c393406570e"
      },
      "source": [
        "# electra with fine-tuning, LSTM \n",
        "bert_model_name = 'electra_base'\n",
        "epochs = 4\n",
        "init_lr = 2e-5\n",
        "batch_size = 8\n",
        "do_fine_tuning = True\n",
        "folds = 2\n",
        "\n",
        "def create_layers(net, text_input): \n",
        "  filter_length = 1\n",
        "  seq_len = 256\n",
        "  net = tf.keras.layers.Reshape((2,384), input_shape=(None,768))(net)\n",
        "  net = tf.keras.layers.LSTM(256, return_sequences=True, activation='tanh')(net)\n",
        "  net = tf.keras.layers.Dense(256, activation = 'relu')(net)\n",
        "  net = tf.keras.layers.Dropout(0.2)(net)\n",
        "  net = tf.keras.layers.Dense(1, activation='sigmoid', name='classifier', \n",
        "                              kernel_regularizer=tf.keras.regularizers.L1L2(l1=0.01, l2=0.01)\n",
        "                              )(net)\n",
        "  return tf.keras.Model(text_input, net)\n",
        "\n",
        "results, history, classifier_model = bert_10_fold(epochs=epochs, \n",
        "                                                  init_lr=init_lr, \n",
        "                                                  batch_size = batch_size, \n",
        "                                                  do_fine_tuning = do_fine_tuning, \n",
        "                                                  create_classifier = create_layers, \n",
        "                                                  bert_model_name = bert_model_name, \n",
        "                                                  folds = folds)\n",
        "\n",
        "print('Average error ', np.mean(results[bert_model_name]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "electra_base\n",
            "FOLD  0\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "383/383 [==============================] - 82s 171ms/step - loss: 0.9145 - binary_accuracy: 0.5109 - val_loss: 0.9039 - val_binary_accuracy: 0.5088\n",
            "Epoch 2/4\n",
            "383/383 [==============================] - 65s 169ms/step - loss: 0.8751 - binary_accuracy: 0.5922 - val_loss: 0.8885 - val_binary_accuracy: 0.5971\n",
            "Epoch 3/4\n",
            "383/383 [==============================] - 65s 169ms/step - loss: 0.7902 - binary_accuracy: 0.7009 - val_loss: 0.9392 - val_binary_accuracy: 0.6324\n",
            "Epoch 4/4\n",
            "383/383 [==============================] - 65s 169ms/step - loss: 0.7081 - binary_accuracy: 0.7840 - val_loss: 0.9698 - val_binary_accuracy: 0.6088\n",
            "43/43 [==============================] - 2s 55ms/step - loss: 0.9698 - binary_accuracy: 0.6088\n",
            "FOLD  1\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "383/383 [==============================] - 84s 172ms/step - loss: 0.9189 - binary_accuracy: 0.4880 - val_loss: 0.9029 - val_binary_accuracy: 0.5162\n",
            "Epoch 2/4\n",
            "383/383 [==============================] - 65s 169ms/step - loss: 0.8874 - binary_accuracy: 0.5645 - val_loss: 0.8353 - val_binary_accuracy: 0.6515\n",
            "Epoch 3/4\n",
            "383/383 [==============================] - 65s 169ms/step - loss: 0.7776 - binary_accuracy: 0.7156 - val_loss: 0.8307 - val_binary_accuracy: 0.6897\n",
            "Epoch 4/4\n",
            "383/383 [==============================] - 65s 169ms/step - loss: 0.6488 - binary_accuracy: 0.8203 - val_loss: 0.8768 - val_binary_accuracy: 0.6809\n",
            "43/43 [==============================] - 2s 54ms/step - loss: 0.8768 - binary_accuracy: 0.6809\n",
            "Average error  0.6448529362678528\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILkqb5-YuI3u"
      },
      "source": [
        "# electra with fine-tuning, RCNN \n",
        "bert_model_name = 'electra_base'\n",
        "epochs = 4\n",
        "init_lr = 2e-5\n",
        "batch_size = 8\n",
        "do_fine_tuning = True\n",
        "folds = 2\n",
        "\n",
        "        \n",
        "def create_layers(net, text_input): \n",
        "  filter_length = 1\n",
        "  seq_len = 256\n",
        "  \n",
        "  l_embedding = tf.keras.layers.Lambda(lambda x: K.concatenate([K.zeros(shape=(K.shape(x)[0], 1, K.shape(x)[-1])), x[:, :-1]]))(net)\n",
        "\n",
        "  r_embedding = tf.keras.layers.Lambda(lambda x: K.concatenate([K.zeros(shape=(K.shape(x)[0], 1, K.shape(x)[-1])), x[:, 1:]]))(net)\n",
        "\n",
        "  l_embedding = tf.keras.layers.Reshape((2,384), input_shape=(None,768))(l_embedding)\n",
        "  r_embedding = tf.keras.layers.Reshape((2,384), input_shape=(None,768))(r_embedding)\n",
        "  forward = tf.keras.layers.LSTM(256, return_sequences=True)(l_embedding) \n",
        "  backward = tf.keras.layers.LSTM(256, return_sequences=True, go_backwards=True)(r_embedding)\n",
        "  backward = tf.keras.layers.Lambda(lambda x: K.reverse(x, axes=1))(backward)\n",
        "  together = [forward, net , backward]\n",
        "  together = tf.keras.layers.Concatenate(axis=2)(together)\n",
        "  semantic = tf.keras.layers.Conv1D(256, kernel_size=(filter_length), activation=\"relu\")(together)\n",
        "  sentence_embed = tf.keras.layers.Lambda(lambda x: K.max(x, axis=1))(semantic)\n",
        "  net = tf.keras.layers.Dense(256, activation = 'relu')(together)\n",
        "  net = tf.keras.layers.Dropout(0.2)(net)\n",
        "  net = tf.keras.layers.Dense(1, activation='sigmoid', name='classifier', \n",
        "                              kernel_regularizer=tf.keras.regularizers.L1L2(l1=0.01, l2=0.01)\n",
        "                              )(net)\n",
        "  return tf.keras.Model(text_input, net)\n",
        "\n",
        "results, history, classifier_model = bert_10_fold(epochs=epochs, \n",
        "                                                  init_lr=init_lr, \n",
        "                                                  batch_size = batch_size, \n",
        "                                                  do_fine_tuning = do_fine_tuning, \n",
        "                                                  create_classifier = create_layers, \n",
        "                                                  bert_model_name = bert_model_name, \n",
        "                                                  folds = folds)\n",
        "\n",
        "print('Average accuracy ', np.mean(results[bert_model_name]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bimf7-inuI_3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9h0CDEKvuL73"
      },
      "source": [
        "# 10 - layer "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9O6TT885u3bh",
        "outputId": "38264cf7-a80e-4e12-cc02-80b21f788547"
      },
      "source": [
        "# bert baseline with fine-tuning\n",
        "bert_model_name = 'bert_en_uncased_L-12_H-768_A-12'\n",
        "epochs = 4\n",
        "init_lr = 2e-5\n",
        "batch_size = 32\n",
        "do_fine_tuning = True\n",
        "folds = 10\n",
        "\n",
        "def create_layers(net, text_input): \n",
        "  net = tf.keras.layers.Dropout(0.1)(net)\n",
        "  net = tf.keras.layers.Dense(1, activation='sigmoid', name='classifier')(net)\n",
        "  return tf.keras.Model(text_input, net)\n",
        "\n",
        "results, history, classifier_model = bert_10_fold(epochs=epochs, \n",
        "                                                  init_lr=init_lr, \n",
        "                                                  batch_size = batch_size, \n",
        "                                                  do_fine_tuning = do_fine_tuning, \n",
        "                                                  create_classifier = create_layers, \n",
        "                                                  bert_model_name = bert_model_name, \n",
        "                                                  folds = folds)\n",
        "\n",
        "print('Average error ', np.mean(results[bert_model_name]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bert_en_uncased_L-12_H-768_A-12\n",
            "FOLD  0\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "96/96 [==============================] - 68s 552ms/step - loss: 0.7043 - binary_accuracy: 0.5136 - val_loss: 0.6827 - val_binary_accuracy: 0.5529\n",
            "Epoch 2/4\n",
            "96/96 [==============================] - 52s 543ms/step - loss: 0.6736 - binary_accuracy: 0.5880 - val_loss: 0.6779 - val_binary_accuracy: 0.5912\n",
            "Epoch 3/4\n",
            "96/96 [==============================] - 52s 544ms/step - loss: 0.5957 - binary_accuracy: 0.6806 - val_loss: 0.7338 - val_binary_accuracy: 0.6029\n",
            "Epoch 4/4\n",
            "96/96 [==============================] - 52s 543ms/step - loss: 0.5077 - binary_accuracy: 0.7543 - val_loss: 0.7238 - val_binary_accuracy: 0.5706\n",
            "11/11 [==============================] - 2s 187ms/step - loss: 0.7238 - binary_accuracy: 0.5706\n",
            "FOLD  1\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "96/96 [==============================] - 67s 551ms/step - loss: 0.7210 - binary_accuracy: 0.5089 - val_loss: 0.6613 - val_binary_accuracy: 0.6059\n",
            "Epoch 2/4\n",
            "96/96 [==============================] - 52s 543ms/step - loss: 0.6433 - binary_accuracy: 0.6407 - val_loss: 0.6255 - val_binary_accuracy: 0.6529\n",
            "Epoch 3/4\n",
            "96/96 [==============================] - 52s 543ms/step - loss: 0.5449 - binary_accuracy: 0.7328 - val_loss: 0.6642 - val_binary_accuracy: 0.6529\n",
            "Epoch 4/4\n",
            "96/96 [==============================] - 52s 543ms/step - loss: 0.4393 - binary_accuracy: 0.8135 - val_loss: 0.6884 - val_binary_accuracy: 0.6382\n",
            "11/11 [==============================] - 2s 188ms/step - loss: 0.6884 - binary_accuracy: 0.6382\n",
            "FOLD  2\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "96/96 [==============================] - 70s 551ms/step - loss: 0.9492 - binary_accuracy: 0.5281 - val_loss: 0.6519 - val_binary_accuracy: 0.6235\n",
            "Epoch 2/4\n",
            "96/96 [==============================] - 52s 543ms/step - loss: 0.6550 - binary_accuracy: 0.6049 - val_loss: 0.6604 - val_binary_accuracy: 0.6294\n",
            "Epoch 3/4\n",
            "96/96 [==============================] - 52s 542ms/step - loss: 0.5406 - binary_accuracy: 0.7190 - val_loss: 0.7430 - val_binary_accuracy: 0.6294\n",
            "Epoch 4/4\n",
            "96/96 [==============================] - 52s 542ms/step - loss: 0.4279 - binary_accuracy: 0.8118 - val_loss: 0.7413 - val_binary_accuracy: 0.6382\n",
            "11/11 [==============================] - 2s 186ms/step - loss: 0.7413 - binary_accuracy: 0.6382\n",
            "FOLD  3\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "96/96 [==============================] - 67s 550ms/step - loss: 0.7734 - binary_accuracy: 0.5493 - val_loss: 0.6494 - val_binary_accuracy: 0.6235\n",
            "Epoch 2/4\n",
            "96/96 [==============================] - 52s 543ms/step - loss: 0.6224 - binary_accuracy: 0.6479 - val_loss: 0.6534 - val_binary_accuracy: 0.6824\n",
            "Epoch 3/4\n",
            "96/96 [==============================] - 52s 543ms/step - loss: 0.5115 - binary_accuracy: 0.7547 - val_loss: 0.6850 - val_binary_accuracy: 0.6500\n",
            "Epoch 4/4\n",
            "96/96 [==============================] - 52s 543ms/step - loss: 0.4113 - binary_accuracy: 0.8161 - val_loss: 0.7243 - val_binary_accuracy: 0.6471\n",
            "11/11 [==============================] - 2s 188ms/step - loss: 0.7243 - binary_accuracy: 0.6471\n",
            "FOLD  4\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "96/96 [==============================] - 71s 551ms/step - loss: 0.6961 - binary_accuracy: 0.5404 - val_loss: 0.6564 - val_binary_accuracy: 0.6147\n",
            "Epoch 2/4\n",
            "96/96 [==============================] - 52s 543ms/step - loss: 0.6361 - binary_accuracy: 0.6371 - val_loss: 0.6548 - val_binary_accuracy: 0.6147\n",
            "Epoch 3/4\n",
            "96/96 [==============================] - 52s 543ms/step - loss: 0.5428 - binary_accuracy: 0.7303 - val_loss: 0.6901 - val_binary_accuracy: 0.6206\n",
            "Epoch 4/4\n",
            "96/96 [==============================] - 52s 544ms/step - loss: 0.4464 - binary_accuracy: 0.7955 - val_loss: 0.7011 - val_binary_accuracy: 0.6353\n",
            "11/11 [==============================] - 2s 187ms/step - loss: 0.7011 - binary_accuracy: 0.6353\n",
            "FOLD  5\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "96/96 [==============================] - 71s 550ms/step - loss: 0.7000 - binary_accuracy: 0.5364 - val_loss: 0.6774 - val_binary_accuracy: 0.5824\n",
            "Epoch 2/4\n",
            "96/96 [==============================] - 52s 544ms/step - loss: 0.6464 - binary_accuracy: 0.6076 - val_loss: 0.6804 - val_binary_accuracy: 0.6059\n",
            "Epoch 3/4\n",
            "96/96 [==============================] - 52s 543ms/step - loss: 0.5582 - binary_accuracy: 0.7198 - val_loss: 0.7553 - val_binary_accuracy: 0.5882\n",
            "Epoch 4/4\n",
            "96/96 [==============================] - 52s 543ms/step - loss: 0.4763 - binary_accuracy: 0.7846 - val_loss: 0.7586 - val_binary_accuracy: 0.5971\n",
            "11/11 [==============================] - 2s 189ms/step - loss: 0.7586 - binary_accuracy: 0.5971\n",
            "FOLD  6\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "96/96 [==============================] - 72s 550ms/step - loss: 0.7196 - binary_accuracy: 0.4901 - val_loss: 0.6527 - val_binary_accuracy: 0.6176\n",
            "Epoch 2/4\n",
            "96/96 [==============================] - 52s 543ms/step - loss: 0.6556 - binary_accuracy: 0.6041 - val_loss: 0.6151 - val_binary_accuracy: 0.6559\n",
            "Epoch 3/4\n",
            "96/96 [==============================] - 52s 543ms/step - loss: 0.5618 - binary_accuracy: 0.7040 - val_loss: 0.6311 - val_binary_accuracy: 0.6559\n",
            "Epoch 4/4\n",
            "96/96 [==============================] - 52s 542ms/step - loss: 0.4887 - binary_accuracy: 0.7639 - val_loss: 0.6288 - val_binary_accuracy: 0.6647\n",
            "11/11 [==============================] - 2s 186ms/step - loss: 0.6288 - binary_accuracy: 0.6647\n",
            "FOLD  7\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "96/96 [==============================] - 68s 551ms/step - loss: 0.7125 - binary_accuracy: 0.5038 - val_loss: 0.6604 - val_binary_accuracy: 0.5853\n",
            "Epoch 2/4\n",
            "96/96 [==============================] - 52s 543ms/step - loss: 0.6591 - binary_accuracy: 0.6077 - val_loss: 0.6332 - val_binary_accuracy: 0.6265\n",
            "Epoch 3/4\n",
            "96/96 [==============================] - 52s 543ms/step - loss: 0.5605 - binary_accuracy: 0.7037 - val_loss: 0.6477 - val_binary_accuracy: 0.6294\n",
            "Epoch 4/4\n",
            "96/96 [==============================] - 52s 543ms/step - loss: 0.4697 - binary_accuracy: 0.7823 - val_loss: 0.6735 - val_binary_accuracy: 0.6147\n",
            "11/11 [==============================] - 2s 185ms/step - loss: 0.6735 - binary_accuracy: 0.6147\n",
            "FOLD  8\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "96/96 [==============================] - 70s 549ms/step - loss: 0.7067 - binary_accuracy: 0.5209 - val_loss: 0.6603 - val_binary_accuracy: 0.5941\n",
            "Epoch 2/4\n",
            "96/96 [==============================] - 52s 542ms/step - loss: 0.6405 - binary_accuracy: 0.6297 - val_loss: 0.6547 - val_binary_accuracy: 0.5912\n",
            "Epoch 3/4\n",
            "96/96 [==============================] - 52s 542ms/step - loss: 0.5609 - binary_accuracy: 0.7065 - val_loss: 0.6657 - val_binary_accuracy: 0.6088\n",
            "Epoch 4/4\n",
            "96/96 [==============================] - 52s 542ms/step - loss: 0.4841 - binary_accuracy: 0.7623 - val_loss: 0.6736 - val_binary_accuracy: 0.6265\n",
            "11/11 [==============================] - 2s 188ms/step - loss: 0.6736 - binary_accuracy: 0.6265\n",
            "FOLD  9\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "96/96 [==============================] - 70s 550ms/step - loss: 0.7340 - binary_accuracy: 0.5272 - val_loss: 0.6669 - val_binary_accuracy: 0.6206\n",
            "Epoch 2/4\n",
            "96/96 [==============================] - 52s 541ms/step - loss: 0.6569 - binary_accuracy: 0.5985 - val_loss: 0.6535 - val_binary_accuracy: 0.6147\n",
            "Epoch 3/4\n",
            "96/96 [==============================] - 52s 542ms/step - loss: 0.5718 - binary_accuracy: 0.7032 - val_loss: 0.6628 - val_binary_accuracy: 0.6382\n",
            "Epoch 4/4\n",
            "96/96 [==============================] - 52s 541ms/step - loss: 0.4935 - binary_accuracy: 0.7753 - val_loss: 0.6809 - val_binary_accuracy: 0.6324\n",
            "11/11 [==============================] - 2s 188ms/step - loss: 0.6809 - binary_accuracy: 0.6324\n",
            "Average error  0.6264705836772919\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQ1KYkNBetuf"
      },
      "source": [
        "# bert baseline with fine-tuning\n",
        "bert_model_name = 'bert_en_uncased_L-12_H-768_A-12'\n",
        "epochs = 2\n",
        "init_lr = 2e-5\n",
        "batch_size = 32\n",
        "do_fine_tuning = True\n",
        "folds = 10\n",
        "\n",
        "def create_layers(net, text_input): \n",
        "  net = tf.keras.layers.Dropout(0.1)(net)\n",
        "  net = tf.keras.layers.Dense(1, activation='sigmoid', name='classifier')(net)\n",
        "  return tf.keras.Model(text_input, net)\n",
        "\n",
        "results, history, classifier_model = bert_10_fold(epochs=epochs, \n",
        "                                                  init_lr=init_lr, \n",
        "                                                  batch_size = batch_size, \n",
        "                                                  do_fine_tuning = do_fine_tuning, \n",
        "                                                  create_classifier = create_layers, \n",
        "                                                  bert_model_name = bert_model_name, \n",
        "                                                  folds = folds)\n",
        "\n",
        "print('Average error ', np.mean(results[bert_model_name]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2gPnMuMF5PM5",
        "outputId": "ed44e57b-f3a2-43f5-ec6c-6c74769ad90c"
      },
      "source": [
        "# electra baseline with fine tuning\n",
        "bert_model_name = 'electra_base'\n",
        "epochs = 4\n",
        "init_lr = 2e-5\n",
        "batch_size = 8\n",
        "do_fine_tuning = True\n",
        "folds = 10\n",
        "\n",
        "def create_layers(net, text_input): \n",
        "  net = tf.keras.layers.Dropout(0.1)(net)\n",
        "  net = tf.keras.layers.Dense(1, activation='sigmoid', name='classifier')(net)\n",
        "  return tf.keras.Model(text_input, net)\n",
        "\n",
        "results1, history1, classifier_model1 = bert_10_fold(epochs=epochs, \n",
        "                                                  init_lr=init_lr, \n",
        "                                                  batch_size = batch_size, \n",
        "                                                  do_fine_tuning = do_fine_tuning, \n",
        "                                                  create_classifier = create_layers, \n",
        "                                                  bert_model_name = bert_model_name, \n",
        "                                                  folds = folds)\n",
        "\n",
        "print('Average error ', np.mean(results1[bert_model_name]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "electra_base\n",
            "FOLD  0\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYOAgsGLevBH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d3a225b-babe-49e0-9b04-78c35a8f9741"
      },
      "source": [
        "# electra baseline with fine tuning\n",
        "bert_model_name = 'electra_base'\n",
        "epochs = 3\n",
        "init_lr = 2e-5\n",
        "batch_size = 8\n",
        "do_fine_tuning = True\n",
        "folds = 10\n",
        "\n",
        "def create_layers(net, text_input): \n",
        "  net = tf.keras.layers.Dropout(0.1)(net)\n",
        "  net = tf.keras.layers.Dense(1, activation='sigmoid', name='classifier')(net)\n",
        "  return tf.keras.Model(text_input, net)\n",
        "\n",
        "results1, history1, classifier_model1 = bert_10_fold(epochs=epochs, \n",
        "                                                  init_lr=init_lr, \n",
        "                                                  batch_size = batch_size, \n",
        "                                                  do_fine_tuning = do_fine_tuning, \n",
        "                                                  create_classifier = create_layers, \n",
        "                                                  bert_model_name = bert_model_name, \n",
        "                                                  folds = folds)\n",
        "\n",
        "print('Average error ', np.mean(results1[bert_model_name]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "electra_base\n",
            "FOLD  0\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "383/383 [==============================] - 78s 164ms/step - loss: 0.7044 - binary_accuracy: 0.5311 - val_loss: 0.6675 - val_binary_accuracy: 0.5941\n",
            "Epoch 2/3\n",
            "383/383 [==============================] - 62s 162ms/step - loss: 0.6088 - binary_accuracy: 0.6652 - val_loss: 0.6844 - val_binary_accuracy: 0.6382\n",
            "Epoch 3/3\n",
            "383/383 [==============================] - 62s 162ms/step - loss: 0.4073 - binary_accuracy: 0.8133 - val_loss: 0.8214 - val_binary_accuracy: 0.6559\n",
            "43/43 [==============================] - 2s 49ms/step - loss: 0.8214 - binary_accuracy: 0.6559\n",
            "FOLD  1\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "383/383 [==============================] - 76s 164ms/step - loss: 0.7094 - binary_accuracy: 0.5363 - val_loss: 0.6305 - val_binary_accuracy: 0.6588\n",
            "Epoch 2/3\n",
            "383/383 [==============================] - 62s 162ms/step - loss: 0.6081 - binary_accuracy: 0.6732 - val_loss: 0.6281 - val_binary_accuracy: 0.7059\n",
            "Epoch 3/3\n",
            "383/383 [==============================] - 62s 162ms/step - loss: 0.4132 - binary_accuracy: 0.8160 - val_loss: 0.6690 - val_binary_accuracy: 0.7029\n",
            "43/43 [==============================] - 2s 50ms/step - loss: 0.6690 - binary_accuracy: 0.7029\n",
            "FOLD  2\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "383/383 [==============================] - 76s 164ms/step - loss: 0.6949 - binary_accuracy: 0.5516 - val_loss: 0.6196 - val_binary_accuracy: 0.6529\n",
            "Epoch 2/3\n",
            "383/383 [==============================] - 62s 162ms/step - loss: 0.5888 - binary_accuracy: 0.6943 - val_loss: 0.6150 - val_binary_accuracy: 0.6735\n",
            "Epoch 3/3\n",
            "383/383 [==============================] - 62s 162ms/step - loss: 0.3995 - binary_accuracy: 0.8298 - val_loss: 0.7380 - val_binary_accuracy: 0.6765\n",
            "43/43 [==============================] - 2s 49ms/step - loss: 0.7380 - binary_accuracy: 0.6765\n",
            "FOLD  3\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "383/383 [==============================] - 78s 164ms/step - loss: 0.7015 - binary_accuracy: 0.5138 - val_loss: 0.6519 - val_binary_accuracy: 0.6382\n",
            "Epoch 2/3\n",
            "383/383 [==============================] - 62s 162ms/step - loss: 0.5967 - binary_accuracy: 0.6844 - val_loss: 0.7141 - val_binary_accuracy: 0.6471\n",
            "Epoch 3/3\n",
            "383/383 [==============================] - 62s 162ms/step - loss: 0.4177 - binary_accuracy: 0.8062 - val_loss: 0.7776 - val_binary_accuracy: 0.6735\n",
            "43/43 [==============================] - 2s 49ms/step - loss: 0.7776 - binary_accuracy: 0.6735\n",
            "FOLD  4\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "383/383 [==============================] - 79s 169ms/step - loss: 0.7033 - binary_accuracy: 0.5277 - val_loss: 0.6543 - val_binary_accuracy: 0.6412\n",
            "Epoch 2/3\n",
            "383/383 [==============================] - 62s 162ms/step - loss: 0.5903 - binary_accuracy: 0.6847 - val_loss: 0.7745 - val_binary_accuracy: 0.6382\n",
            "Epoch 3/3\n",
            "383/383 [==============================] - 62s 162ms/step - loss: 0.4067 - binary_accuracy: 0.8245 - val_loss: 0.8203 - val_binary_accuracy: 0.6412\n",
            "43/43 [==============================] - 2s 49ms/step - loss: 0.8203 - binary_accuracy: 0.6412\n",
            "FOLD  5\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "383/383 [==============================] - 79s 164ms/step - loss: 0.7089 - binary_accuracy: 0.5136 - val_loss: 0.6644 - val_binary_accuracy: 0.5882\n",
            "Epoch 2/3\n",
            "383/383 [==============================] - 62s 163ms/step - loss: 0.5893 - binary_accuracy: 0.6825 - val_loss: 0.7470 - val_binary_accuracy: 0.6206\n",
            "Epoch 3/3\n",
            "383/383 [==============================] - 62s 162ms/step - loss: 0.4133 - binary_accuracy: 0.8135 - val_loss: 0.8643 - val_binary_accuracy: 0.6147\n",
            "43/43 [==============================] - 2s 49ms/step - loss: 0.8643 - binary_accuracy: 0.6147\n",
            "FOLD  6\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "383/383 [==============================] - 78s 164ms/step - loss: 0.7018 - binary_accuracy: 0.5098 - val_loss: 0.6333 - val_binary_accuracy: 0.6559\n",
            "Epoch 2/3\n",
            "383/383 [==============================] - 62s 162ms/step - loss: 0.6003 - binary_accuracy: 0.6737 - val_loss: 0.6030 - val_binary_accuracy: 0.6941\n",
            "Epoch 3/3\n",
            "383/383 [==============================] - 62s 162ms/step - loss: 0.4217 - binary_accuracy: 0.8062 - val_loss: 0.6583 - val_binary_accuracy: 0.6882\n",
            "43/43 [==============================] - 2s 50ms/step - loss: 0.6583 - binary_accuracy: 0.6882\n",
            "FOLD  7\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "383/383 [==============================] - 77s 164ms/step - loss: 0.7035 - binary_accuracy: 0.5454 - val_loss: 0.6366 - val_binary_accuracy: 0.6441\n",
            "Epoch 2/3\n",
            "383/383 [==============================] - 62s 162ms/step - loss: 0.5965 - binary_accuracy: 0.6838 - val_loss: 0.6865 - val_binary_accuracy: 0.6294\n",
            "Epoch 3/3\n",
            "383/383 [==============================] - 62s 162ms/step - loss: 0.4340 - binary_accuracy: 0.8023 - val_loss: 0.8140 - val_binary_accuracy: 0.6471\n",
            "43/43 [==============================] - 2s 49ms/step - loss: 0.8140 - binary_accuracy: 0.6471\n",
            "FOLD  8\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "383/383 [==============================] - 77s 164ms/step - loss: 0.7042 - binary_accuracy: 0.5223 - val_loss: 0.6506 - val_binary_accuracy: 0.6353\n",
            "Epoch 2/3\n",
            "383/383 [==============================] - 62s 162ms/step - loss: 0.6070 - binary_accuracy: 0.6704 - val_loss: 0.7311 - val_binary_accuracy: 0.6059\n",
            "Epoch 3/3\n",
            "383/383 [==============================] - 62s 162ms/step - loss: 0.4207 - binary_accuracy: 0.8055 - val_loss: 0.8084 - val_binary_accuracy: 0.6618\n",
            "43/43 [==============================] - 2s 50ms/step - loss: 0.8084 - binary_accuracy: 0.6618\n",
            "FOLD  9\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "383/383 [==============================] - 78s 164ms/step - loss: 0.6978 - binary_accuracy: 0.5398 - val_loss: 0.6525 - val_binary_accuracy: 0.6118\n",
            "Epoch 2/3\n",
            "383/383 [==============================] - 62s 162ms/step - loss: 0.5896 - binary_accuracy: 0.6912 - val_loss: 0.7486 - val_binary_accuracy: 0.6324\n",
            "Epoch 3/3\n",
            "383/383 [==============================] - 62s 162ms/step - loss: 0.4085 - binary_accuracy: 0.8208 - val_loss: 0.8845 - val_binary_accuracy: 0.5882\n",
            "43/43 [==============================] - 2s 51ms/step - loss: 0.8845 - binary_accuracy: 0.5882\n",
            "Average error  0.6549999952316284\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrZzEDO2evnm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 494
        },
        "outputId": "763ea89c-07a4-4f92-c4ac-4698d3914eec"
      },
      "source": [
        "# electra baseline with fine tuning\n",
        "bert_model_name = 'electra_base'\n",
        "epochs = 2\n",
        "init_lr = 2e-5\n",
        "batch_size = 8\n",
        "do_fine_tuning = True\n",
        "folds = 10\n",
        "\n",
        "def create_layers(net, text_input): \n",
        "  net = tf.keras.layers.Dropout(0.1)(net)\n",
        "  net = tf.keras.layers.Dense(1, activation='sigmoid', name='classifier')(net)\n",
        "  return tf.keras.Model(text_input, net)\n",
        "\n",
        "results1, history1, classifier_model1 = bert_10_fold(epochs=epochs, \n",
        "                                                  init_lr=init_lr, \n",
        "                                                  batch_size = batch_size, \n",
        "                                                  do_fine_tuning = do_fine_tuning, \n",
        "                                                  create_classifier = create_layers, \n",
        "                                                  bert_model_name = bert_model_name, \n",
        "                                                  folds = folds)\n",
        "\n",
        "print('Average error ', np.mean(results1[bert_model_name]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "electra_base\n",
            "FOLD  0\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-c2e02f1640ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m                                                   \u001b[0mcreate_classifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                                                   \u001b[0mbert_model_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert_model_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m                                                   folds = folds)\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Average error '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbert_model_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-cded5a533f20>\u001b[0m in \u001b[0;36mbert_10_fold\u001b[0;34m(epochs, init_lr, batch_size, create_classifier, bert_model_name, do_fine_tuning, folds)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# fit model and calculate accuracy on test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0maccuracies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    869\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    724\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    725\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 726\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2967\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2968\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2969\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2970\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3204\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3205\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3206\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3207\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3208\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    971\u001b[0m                     \u001b[0mrecursive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m                     \u001b[0moptional_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mautograph_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m                     \u001b[0muser_requested\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    974\u001b[0m                 ))\n\u001b[1;32m    975\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    457\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverted_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0meffective_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverted_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0meffective_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     14\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_requested\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_allowlisted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m   \u001b[0;31m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    477\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 479\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mstep_function\u001b[0;34m(model, iterator)\u001b[0m\n\u001b[1;32m    793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m       \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 795\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    796\u001b[0m       outputs = reduce_per_replica(\n\u001b[1;32m    797\u001b[0m           outputs, self.distribute_strategy, reduction='first')\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1257\u001b[0m       fn = autograph.tf_convert(\n\u001b[1;32m   1258\u001b[0m           fn, autograph_ctx.control_status_ctx(), convert_by_default=False)\n\u001b[0;32m-> 1259\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extended\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mcall_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   2728\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2729\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2730\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2732\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m_call_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   3415\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3416\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mReplicaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplica_id_in_sync_group\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3417\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3419\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_reduce_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestinations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    665\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconversion_ctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    668\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_requested\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_allowlisted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m   \u001b[0;31m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 478\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    479\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mrun_step\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mrun_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 788\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    789\u001b[0m         \u001b[0;31m# Ensure counter is updated only if `train_step` succeeds.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_minimum_control_deps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    755\u001b[0m       loss = self.compiled_loss(\n\u001b[1;32m    756\u001b[0m           y, y_pred, sample_weight, regularization_losses=self.losses)\n\u001b[0;32m--> 757\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompiled_metrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(self, loss, var_list, grad_loss, name, tape)\u001b[0m\n\u001b[1;32m    495\u001b[0m     \"\"\"\n\u001b[1;32m    496\u001b[0m     grads_and_vars = self._compute_gradients(\n\u001b[0;32m--> 497\u001b[0;31m         loss, var_list=var_list, grad_loss=grad_loss, tape=tape)\n\u001b[0m\u001b[1;32m    498\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36m_compute_gradients\u001b[0;34m(self, loss, var_list, grad_loss, tape)\u001b[0m\n\u001b[1;32m    546\u001b[0m     \u001b[0mvar_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/gradients\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m       \u001b[0mgrads_and_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m     self._assert_valid_dtypes([\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36m_get_gradients\u001b[0;34m(self, tape, loss, var_list, grad_loss)\u001b[0m\n\u001b[1;32m    439\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_get_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[0;34m\"\"\"Called in `minimize` to compute gradients from loss.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m     \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1085\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1086\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1087\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1088\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_backward_function\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    805\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_backward_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0mcall_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rewrite_forward_and_call_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_backward_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_rewrite_forward_and_call_backward\u001b[0;34m(self, op, *doutputs)\u001b[0m\n\u001b[1;32m    720\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_rewrite_forward_and_call_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mdoutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m     \u001b[0;34m\"\"\"Add outputs to the forward call and feed them to the grad function.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m     \u001b[0mforward_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackwards_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbackwards_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mbackwards_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstructured_outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mforward_backward\u001b[0;34m(self, num_doutputs)\u001b[0m\n\u001b[1;32m    629\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mforward_backward\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mforward_backward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m     \u001b[0mforward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_construct_forward_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_doutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cached_function_pairs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum_doutputs\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_construct_forward_backward\u001b[0;34m(self, num_doutputs)\u001b[0m\n\u001b[1;32m    677\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m           \u001b[0msignature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 679\u001b[0;31m           func_graph=backwards_graph)\n\u001b[0m\u001b[1;32m    680\u001b[0m       \u001b[0mbackwards_graph_captures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackwards_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexternal_captures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m       captures_from_forward = [\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_backprop_function\u001b[0;34m(*grad_ys)\u001b[0m\n\u001b[1;32m    667\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0mgrad_ys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrad_ys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m             src_graph=self._func_graph)\n\u001b[0m\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gradients_util.py\u001b[0m in \u001b[0;36m_GradientsHelper\u001b[0;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)\u001b[0m\n\u001b[1;32m    682\u001b[0m                 \u001b[0;31m# functions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m                 in_grads = _MaybeCompile(grad_scope, op, func_call,\n\u001b[0;32m--> 684\u001b[0;31m                                          lambda: grad_fn(op, *out_grads))\n\u001b[0m\u001b[1;32m    685\u001b[0m               \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m                 \u001b[0;31m# For function call ops, we add a 'SymbolicGradient'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gradients_util.py\u001b[0m in \u001b[0;36m_MaybeCompile\u001b[0;34m(scope, op, func, grad_fn)\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mxla_compile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Exit early\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m   \u001b[0;31m# If the gradients are supposed to be compiled separately, we give them a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gradients_util.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    682\u001b[0m                 \u001b[0;31m# functions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m                 in_grads = _MaybeCompile(grad_scope, op, func_call,\n\u001b[0;32m--> 684\u001b[0;31m                                          lambda: grad_fn(op, *out_grads))\n\u001b[0m\u001b[1;32m    685\u001b[0m               \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m                 \u001b[0;31m# For function call ops, we add a 'SymbolicGradient'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_rewrite_forward_and_call_backward\u001b[0;34m(self, op, *doutputs)\u001b[0m\n\u001b[1;32m    720\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_rewrite_forward_and_call_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mdoutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m     \u001b[0;34m\"\"\"Add outputs to the forward call and feed them to the grad function.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m     \u001b[0mforward_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackwards_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbackwards_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mbackwards_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstructured_outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mforward_backward\u001b[0;34m(self, num_doutputs)\u001b[0m\n\u001b[1;32m    629\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mforward_backward\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mforward_backward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m     \u001b[0mforward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_construct_forward_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_doutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cached_function_pairs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum_doutputs\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_construct_forward_backward\u001b[0;34m(self, num_doutputs)\u001b[0m\n\u001b[1;32m    677\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m           \u001b[0msignature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 679\u001b[0;31m           func_graph=backwards_graph)\n\u001b[0m\u001b[1;32m    680\u001b[0m       \u001b[0mbackwards_graph_captures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackwards_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexternal_captures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m       captures_from_forward = [\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_backprop_function\u001b[0;34m(*grad_ys)\u001b[0m\n\u001b[1;32m    667\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0mgrad_ys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrad_ys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m             src_graph=self._func_graph)\n\u001b[0m\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gradients_util.py\u001b[0m in \u001b[0;36m_GradientsHelper\u001b[0;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)\u001b[0m\n\u001b[1;32m    682\u001b[0m                 \u001b[0;31m# functions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m                 in_grads = _MaybeCompile(grad_scope, op, func_call,\n\u001b[0;32m--> 684\u001b[0;31m                                          lambda: grad_fn(op, *out_grads))\n\u001b[0m\u001b[1;32m    685\u001b[0m               \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m                 \u001b[0;31m# For function call ops, we add a 'SymbolicGradient'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gradients_util.py\u001b[0m in \u001b[0;36m_MaybeCompile\u001b[0;34m(scope, op, func, grad_fn)\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mxla_compile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Exit early\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m   \u001b[0;31m# If the gradients are supposed to be compiled separately, we give them a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gradients_util.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    682\u001b[0m                 \u001b[0;31m# functions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m                 in_grads = _MaybeCompile(grad_scope, op, func_call,\n\u001b[0;32m--> 684\u001b[0;31m                                          lambda: grad_fn(op, *out_grads))\n\u001b[0m\u001b[1;32m    685\u001b[0m               \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m                 \u001b[0;31m# For function call ops, we add a 'SymbolicGradient'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_rewrite_forward_and_call_backward\u001b[0;34m(self, op, *doutputs)\u001b[0m\n\u001b[1;32m    720\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_rewrite_forward_and_call_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mdoutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m     \u001b[0;34m\"\"\"Add outputs to the forward call and feed them to the grad function.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m     \u001b[0mforward_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackwards_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbackwards_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mbackwards_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstructured_outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mforward_backward\u001b[0;34m(self, num_doutputs)\u001b[0m\n\u001b[1;32m    629\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mforward_backward\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mforward_backward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m     \u001b[0mforward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_construct_forward_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_doutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cached_function_pairs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum_doutputs\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_construct_forward_backward\u001b[0;34m(self, num_doutputs)\u001b[0m\n\u001b[1;32m    677\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m           \u001b[0msignature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 679\u001b[0;31m           func_graph=backwards_graph)\n\u001b[0m\u001b[1;32m    680\u001b[0m       \u001b[0mbackwards_graph_captures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackwards_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexternal_captures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m       captures_from_forward = [\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_backprop_function\u001b[0;34m(*grad_ys)\u001b[0m\n\u001b[1;32m    667\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0mgrad_ys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrad_ys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m             src_graph=self._func_graph)\n\u001b[0m\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gradients_util.py\u001b[0m in \u001b[0;36m_GradientsHelper\u001b[0;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)\u001b[0m\n\u001b[1;32m    682\u001b[0m                 \u001b[0;31m# functions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m                 in_grads = _MaybeCompile(grad_scope, op, func_call,\n\u001b[0;32m--> 684\u001b[0;31m                                          lambda: grad_fn(op, *out_grads))\n\u001b[0m\u001b[1;32m    685\u001b[0m               \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m                 \u001b[0;31m# For function call ops, we add a 'SymbolicGradient'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gradients_util.py\u001b[0m in \u001b[0;36m_MaybeCompile\u001b[0;34m(scope, op, func, grad_fn)\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mxla_compile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Exit early\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m   \u001b[0;31m# If the gradients are supposed to be compiled separately, we give them a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gradients_util.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    682\u001b[0m                 \u001b[0;31m# functions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m                 in_grads = _MaybeCompile(grad_scope, op, func_call,\n\u001b[0;32m--> 684\u001b[0;31m                                          lambda: grad_fn(op, *out_grads))\n\u001b[0m\u001b[1;32m    685\u001b[0m               \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m                 \u001b[0;31m# For function call ops, we add a 'SymbolicGradient'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_rewrite_forward_and_call_backward\u001b[0;34m(self, op, *doutputs)\u001b[0m\n\u001b[1;32m    720\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_rewrite_forward_and_call_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mdoutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m     \u001b[0;34m\"\"\"Add outputs to the forward call and feed them to the grad function.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m     \u001b[0mforward_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackwards_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbackwards_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mbackwards_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstructured_outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mforward_backward\u001b[0;34m(self, num_doutputs)\u001b[0m\n\u001b[1;32m    629\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mforward_backward\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mforward_backward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m     \u001b[0mforward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_construct_forward_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_doutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cached_function_pairs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum_doutputs\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_construct_forward_backward\u001b[0;34m(self, num_doutputs)\u001b[0m\n\u001b[1;32m    677\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m           \u001b[0msignature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 679\u001b[0;31m           func_graph=backwards_graph)\n\u001b[0m\u001b[1;32m    680\u001b[0m       \u001b[0mbackwards_graph_captures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackwards_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexternal_captures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m       captures_from_forward = [\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_backprop_function\u001b[0;34m(*grad_ys)\u001b[0m\n\u001b[1;32m    667\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0mgrad_ys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrad_ys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m             src_graph=self._func_graph)\n\u001b[0m\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gradients_util.py\u001b[0m in \u001b[0;36m_GradientsHelper\u001b[0;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)\u001b[0m\n\u001b[1;32m    682\u001b[0m                 \u001b[0;31m# functions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m                 in_grads = _MaybeCompile(grad_scope, op, func_call,\n\u001b[0;32m--> 684\u001b[0;31m                                          lambda: grad_fn(op, *out_grads))\n\u001b[0m\u001b[1;32m    685\u001b[0m               \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m                 \u001b[0;31m# For function call ops, we add a 'SymbolicGradient'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gradients_util.py\u001b[0m in \u001b[0;36m_MaybeCompile\u001b[0;34m(scope, op, func, grad_fn)\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mxla_compile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Exit early\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m   \u001b[0;31m# If the gradients are supposed to be compiled separately, we give them a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gradients_util.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    682\u001b[0m                 \u001b[0;31m# functions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m                 in_grads = _MaybeCompile(grad_scope, op, func_call,\n\u001b[0;32m--> 684\u001b[0;31m                                          lambda: grad_fn(op, *out_grads))\n\u001b[0m\u001b[1;32m    685\u001b[0m               \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m                 \u001b[0;31m# For function call ops, we add a 'SymbolicGradient'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/math_grad.py\u001b[0m in \u001b[0;36m_MulGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m   1335\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m     gx = array_ops.reshape(\n\u001b[0;32m-> 1337\u001b[0;31m         math_ops.reduce_sum(gen_math_ops.mul(grad, y), rx), sx)\n\u001b[0m\u001b[1;32m   1338\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mskip_input_indices\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mskip_input_indices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m     \u001b[0mgy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmul\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   6076\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6077\u001b[0m   _, _, _op, _outputs = _op_def_library._apply_op_helper(\n\u001b[0;32m-> 6078\u001b[0;31m         \"Mul\", x=x, y=y, name=name)\n\u001b[0m\u001b[1;32m   6079\u001b[0m   \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6080\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    748\u001b[0m       op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n\u001b[1;32m    749\u001b[0m                                  \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 750\u001b[0;31m                                  attrs=attr_protos, op_def=op_def)\n\u001b[0m\u001b[1;32m    751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m     \u001b[0;31m# `outputs` is returned as a separate return value so that the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m    590\u001b[0m     return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n\u001b[1;32m    591\u001b[0m         \u001b[0mop_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 592\u001b[0;31m         compute_device)\n\u001b[0m\u001b[1;32m    593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m   3534\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3535\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3536\u001b[0;31m           op_def=op_def)\n\u001b[0m\u001b[1;32m   3537\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3538\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   2026\u001b[0m       \u001b[0mtf_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2027\u001b[0m       \u001b[0moutput_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpywrap_tf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_OperationOutputType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2028\u001b[0;31m       \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_with_tf_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_output\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2029\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_with_tf_output\u001b[0;34m(op, value_index, dtype, tf_output)\u001b[0m\n\u001b[1;32m    388\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_create_with_tf_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m     \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tf_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, op, value_index, dtype)\u001b[0m\n\u001b[1;32m    375\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m     \u001b[0;31m# This will be set by self._as_tf_output().\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tf_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py\u001b[0m in \u001b[0;36mas_dtype\u001b[0;34m(type_value)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_ANY_TO_TF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype_value\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mKeyError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;31m# TypeError indicates that type_value is not hashable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAZ52f9ObOXh"
      },
      "source": [
        "# electra baseline with fine tuning\n",
        "bert_model_name = 'electra_base'\n",
        "epochs = 3\n",
        "init_lr = 2e-5\n",
        "batch_size = 8\n",
        "do_fine_tuning = True\n",
        "folds = 10\n",
        "\n",
        "def create_layers(net, text_input): \n",
        "  net = tf.keras.layers.Dropout(0.1)(net)\n",
        "  net = tf.keras.layers.Dense(1, activation='sigmoid', name='classifier')(net)\n",
        "  return tf.keras.Model(text_input, net)\n",
        "\n",
        "results1, history1, classifier_model1 = bert_10_fold(epochs=epochs, \n",
        "                                                  init_lr=init_lr, \n",
        "                                                  batch_size = batch_size, \n",
        "                                                  do_fine_tuning = do_fine_tuning, \n",
        "                                                  create_classifier = create_layers, \n",
        "                                                  bert_model_name = bert_model_name, \n",
        "                                                  folds = folds)\n",
        "\n",
        "print('Average error ', np.mean(results1[bert_model_name]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIm8KqWwezNP"
      },
      "source": [
        "# electra with fine-tuning, reg \n",
        "bert_model_name = 'electra_base'\n",
        "epochs = 3\n",
        "init_lr = 2e-5\n",
        "batch_size = 8\n",
        "do_fine_tuning = True\n",
        "folds = 10\n",
        "\n",
        "def create_layers(net, text_input): \n",
        "  net = tf.keras.layers.Dropout(0.1)(net)\n",
        "  net = tf.keras.layers.Dense(1, activation='sigmoid', name='classifier', \n",
        "                              kernel_regularizer=tf.keras.regularizers.L1L2(l1=0.01, l2=0.01)\n",
        "                              )(net)\n",
        "  return tf.keras.Model(text_input, net)\n",
        "\n",
        "results2, history2, classifier_model2 = bert_10_fold(epochs=epochs, \n",
        "                                                  init_lr=init_lr, \n",
        "                                                  batch_size = batch_size, \n",
        "                                                  do_fine_tuning = do_fine_tuning, \n",
        "                                                  create_classifier = create_layers, \n",
        "                                                  bert_model_name = bert_model_name, \n",
        "                                                  folds = folds)\n",
        "\n",
        "print('Average accuracy ', np.mean(results2[bert_model_name]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4fqdDfSvCfR",
        "outputId": "4d77bed1-fb25-4e9f-8638-135fa267b792"
      },
      "source": [
        "# electra with fine-tuning, reg \n",
        "bert_model_name = 'electra_base'\n",
        "epochs = 4\n",
        "init_lr = 2e-5\n",
        "batch_size = 8\n",
        "do_fine_tuning = True\n",
        "folds = 10\n",
        "\n",
        "def create_layers(net, text_input): \n",
        "  net = tf.keras.layers.Dropout(0.1)(net)\n",
        "  net = tf.keras.layers.Dense(1, activation='sigmoid', name='classifier', \n",
        "                              kernel_regularizer=tf.keras.regularizers.L1L2(l1=0.01, l2=0.01)\n",
        "                              )(net)\n",
        "  return tf.keras.Model(text_input, net)\n",
        "\n",
        "results2, history2, classifier_model2 = bert_10_fold(epochs=epochs, \n",
        "                                                  init_lr=init_lr, \n",
        "                                                  batch_size = batch_size, \n",
        "                                                  do_fine_tuning = do_fine_tuning, \n",
        "                                                  create_classifier = create_layers, \n",
        "                                                  bert_model_name = bert_model_name, \n",
        "                                                  folds = folds)\n",
        "\n",
        "print('Average accuracy ', np.mean(results2[bert_model_name]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "electra_base\n",
            "FOLD  0\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "383/383 [==============================] - 81s 166ms/step - loss: 1.0742 - binary_accuracy: 0.4957 - val_loss: 1.0306 - val_binary_accuracy: 0.5853\n",
            "Epoch 2/4\n",
            "383/383 [==============================] - 63s 164ms/step - loss: 0.9678 - binary_accuracy: 0.6564 - val_loss: 1.0262 - val_binary_accuracy: 0.6412\n",
            "Epoch 3/4\n",
            "383/383 [==============================] - 63s 164ms/step - loss: 0.7994 - binary_accuracy: 0.7912 - val_loss: 1.2416 - val_binary_accuracy: 0.6471\n",
            "Epoch 4/4\n",
            "383/383 [==============================] - 63s 164ms/step - loss: 0.6106 - binary_accuracy: 0.8995 - val_loss: 1.4047 - val_binary_accuracy: 0.6471\n",
            "43/43 [==============================] - 2s 51ms/step - loss: 1.4047 - binary_accuracy: 0.6471\n",
            "FOLD  1\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "383/383 [==============================] - 78s 166ms/step - loss: 1.0949 - binary_accuracy: 0.5059 - val_loss: 1.0099 - val_binary_accuracy: 0.6265\n",
            "Epoch 2/4\n",
            "383/383 [==============================] - 63s 164ms/step - loss: 0.9725 - binary_accuracy: 0.6645 - val_loss: 1.0188 - val_binary_accuracy: 0.6882\n",
            "Epoch 3/4\n",
            "383/383 [==============================] - 63s 164ms/step - loss: 0.7676 - binary_accuracy: 0.8092 - val_loss: 1.2658 - val_binary_accuracy: 0.6971\n",
            "Epoch 4/4\n",
            "383/383 [==============================] - 63s 164ms/step - loss: 0.5864 - binary_accuracy: 0.9130 - val_loss: 1.3757 - val_binary_accuracy: 0.6941\n",
            "43/43 [==============================] - 2s 51ms/step - loss: 1.3757 - binary_accuracy: 0.6941\n",
            "FOLD  2\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "383/383 [==============================] - 81s 166ms/step - loss: 1.0529 - binary_accuracy: 0.5217 - val_loss: 0.9744 - val_binary_accuracy: 0.6559\n",
            "Epoch 2/4\n",
            "383/383 [==============================] - 63s 164ms/step - loss: 0.9628 - binary_accuracy: 0.6496 - val_loss: 1.0392 - val_binary_accuracy: 0.6353\n",
            "Epoch 3/4\n",
            "383/383 [==============================] - 63s 164ms/step - loss: 0.7700 - binary_accuracy: 0.8013 - val_loss: 1.2256 - val_binary_accuracy: 0.6794\n",
            "Epoch 4/4\n",
            "383/383 [==============================] - 63s 164ms/step - loss: 0.6111 - binary_accuracy: 0.8890 - val_loss: 1.3891 - val_binary_accuracy: 0.6441\n",
            "43/43 [==============================] - 2s 50ms/step - loss: 1.3891 - binary_accuracy: 0.6441\n",
            "FOLD  3\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "383/383 [==============================] - 81s 166ms/step - loss: 1.0612 - binary_accuracy: 0.5227 - val_loss: 1.0052 - val_binary_accuracy: 0.6412\n",
            "Epoch 2/4\n",
            "383/383 [==============================] - 63s 164ms/step - loss: 0.9606 - binary_accuracy: 0.6631 - val_loss: 1.0818 - val_binary_accuracy: 0.6647\n",
            "Epoch 3/4\n",
            "383/383 [==============================] - 62s 163ms/step - loss: 0.7653 - binary_accuracy: 0.8110 - val_loss: 1.2882 - val_binary_accuracy: 0.6794\n",
            "Epoch 4/4\n",
            "383/383 [==============================] - 63s 163ms/step - loss: 0.6120 - binary_accuracy: 0.8936 - val_loss: 1.3770 - val_binary_accuracy: 0.6853\n",
            "43/43 [==============================] - 2s 50ms/step - loss: 1.3770 - binary_accuracy: 0.6853\n",
            "FOLD  4\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "383/383 [==============================] - 78s 165ms/step - loss: 1.0830 - binary_accuracy: 0.4922 - val_loss: 1.0094 - val_binary_accuracy: 0.6147\n",
            "Epoch 2/4\n",
            "383/383 [==============================] - 63s 163ms/step - loss: 0.9705 - binary_accuracy: 0.6577 - val_loss: 1.0385 - val_binary_accuracy: 0.6588\n",
            "Epoch 3/4\n",
            "383/383 [==============================] - 63s 163ms/step - loss: 0.7860 - binary_accuracy: 0.7942 - val_loss: 1.2128 - val_binary_accuracy: 0.6500\n",
            "Epoch 4/4\n",
            "383/383 [==============================] - 63s 163ms/step - loss: 0.6100 - binary_accuracy: 0.8915 - val_loss: 1.4358 - val_binary_accuracy: 0.6147\n",
            "43/43 [==============================] - 2s 50ms/step - loss: 1.4358 - binary_accuracy: 0.6147\n",
            "FOLD  5\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "383/383 [==============================] - 77s 165ms/step - loss: 1.0714 - binary_accuracy: 0.5179 - val_loss: 1.0090 - val_binary_accuracy: 0.6088\n",
            "Epoch 2/4\n",
            "383/383 [==============================] - 63s 163ms/step - loss: 0.9489 - binary_accuracy: 0.6740 - val_loss: 1.1120 - val_binary_accuracy: 0.6029\n",
            "Epoch 3/4\n",
            "383/383 [==============================] - 63s 163ms/step - loss: 0.7386 - binary_accuracy: 0.8169 - val_loss: 1.6623 - val_binary_accuracy: 0.6176\n",
            "Epoch 4/4\n",
            "383/383 [==============================] - 63s 163ms/step - loss: 0.5787 - binary_accuracy: 0.9078 - val_loss: 1.6289 - val_binary_accuracy: 0.6618\n",
            "43/43 [==============================] - 2s 50ms/step - loss: 1.6289 - binary_accuracy: 0.6618\n",
            "FOLD  6\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "383/383 [==============================] - 82s 165ms/step - loss: 1.0867 - binary_accuracy: 0.5059 - val_loss: 0.9859 - val_binary_accuracy: 0.6441\n",
            "Epoch 2/4\n",
            "383/383 [==============================] - 63s 163ms/step - loss: 0.9459 - binary_accuracy: 0.6810 - val_loss: 1.0375 - val_binary_accuracy: 0.6147\n",
            "Epoch 3/4\n",
            "383/383 [==============================] - 63s 163ms/step - loss: 0.7418 - binary_accuracy: 0.8251 - val_loss: 1.1243 - val_binary_accuracy: 0.6912\n",
            "Epoch 4/4\n",
            "383/383 [==============================] - 63s 163ms/step - loss: 0.6064 - binary_accuracy: 0.8959 - val_loss: 1.3393 - val_binary_accuracy: 0.6618\n",
            "43/43 [==============================] - 2s 50ms/step - loss: 1.3393 - binary_accuracy: 0.6618\n",
            "FOLD  7\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "383/383 [==============================] - 78s 165ms/step - loss: 1.0564 - binary_accuracy: 0.5411 - val_loss: 0.9607 - val_binary_accuracy: 0.6500\n",
            "Epoch 2/4\n",
            "383/383 [==============================] - 62s 163ms/step - loss: 0.9474 - binary_accuracy: 0.6695 - val_loss: 1.0021 - val_binary_accuracy: 0.6559\n",
            "Epoch 3/4\n",
            "383/383 [==============================] - 62s 163ms/step - loss: 0.7607 - binary_accuracy: 0.8107 - val_loss: 1.1977 - val_binary_accuracy: 0.6794\n",
            "Epoch 4/4\n",
            "383/383 [==============================] - 62s 163ms/step - loss: 0.5803 - binary_accuracy: 0.9072 - val_loss: 1.4072 - val_binary_accuracy: 0.6647\n",
            "43/43 [==============================] - 2s 50ms/step - loss: 1.4072 - binary_accuracy: 0.6647\n",
            "FOLD  8\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "383/383 [==============================] - 77s 164ms/step - loss: 1.0627 - binary_accuracy: 0.5221 - val_loss: 1.0008 - val_binary_accuracy: 0.6206\n",
            "Epoch 2/4\n",
            "383/383 [==============================] - 62s 163ms/step - loss: 0.9568 - binary_accuracy: 0.6723 - val_loss: 1.0724 - val_binary_accuracy: 0.6412\n",
            "Epoch 3/4\n",
            "383/383 [==============================] - 63s 163ms/step - loss: 0.7763 - binary_accuracy: 0.8028 - val_loss: 1.4118 - val_binary_accuracy: 0.6294\n",
            "Epoch 4/4\n",
            "383/383 [==============================] - 63s 163ms/step - loss: 0.6114 - binary_accuracy: 0.8970 - val_loss: 1.4072 - val_binary_accuracy: 0.6735\n",
            "43/43 [==============================] - 2s 50ms/step - loss: 1.4072 - binary_accuracy: 0.6735\n",
            "FOLD  9\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "383/383 [==============================] - 78s 165ms/step - loss: 1.0801 - binary_accuracy: 0.5071 - val_loss: 1.0144 - val_binary_accuracy: 0.5824\n",
            "Epoch 2/4\n",
            "383/383 [==============================] - 63s 163ms/step - loss: 0.9634 - binary_accuracy: 0.6724 - val_loss: 1.0781 - val_binary_accuracy: 0.6353\n",
            "Epoch 3/4\n",
            "383/383 [==============================] - 63s 163ms/step - loss: 0.7729 - binary_accuracy: 0.8087 - val_loss: 1.3211 - val_binary_accuracy: 0.6382\n",
            "Epoch 4/4\n",
            "383/383 [==============================] - 63s 163ms/step - loss: 0.5823 - binary_accuracy: 0.9086 - val_loss: 1.6064 - val_binary_accuracy: 0.6265\n",
            "43/43 [==============================] - 2s 50ms/step - loss: 1.6064 - binary_accuracy: 0.6265\n",
            "Average accuracy  0.6573529303073883\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXCHEUhEi9Mk",
        "outputId": "b149a177-a7d6-4680-f0e3-cdb856491dde"
      },
      "source": [
        "# electra with fine-tuning, best CNN \n",
        "bert_model_name = 'electra_base'\n",
        "epochs = 4\n",
        "init_lr = 2e-5\n",
        "batch_size = 8\n",
        "do_fine_tuning = True\n",
        "folds = 10\n",
        "\n",
        "def create_layers(net, text_input): \n",
        "  filter_length = 1\n",
        "  seq_len = 256\n",
        "  net = tf.keras.layers.Reshape((2,384), input_shape=(None,768))(net)\n",
        "  net = tf.keras.layers.Conv1D(filters=256, kernel_size=(filter_length), padding='valid',strides=1, activation='relu')(net)\n",
        "  net = tf.keras.layers.MaxPool1D(pool_size=2)(net)\n",
        "  net = tf.keras.layers.Flatten()(net)\n",
        "  net = tf.keras.layers.Dense(256, activation = 'relu')(net)\n",
        "  net = tf.keras.layers.Dropout(0.1)(net)\n",
        "  net = tf.keras.layers.Dense(1, activation='sigmoid', name='classifier', \n",
        "                              kernel_regularizer=tf.keras.regularizers.L1L2(l1=0.01, l2=0.02)\n",
        "                              )(net)\n",
        "  return tf.keras.Model(text_input, net)\n",
        "\n",
        "results3, history3, classifier_model3 = bert_10_fold(epochs=epochs, \n",
        "                                                  init_lr=init_lr, \n",
        "                                                  batch_size = batch_size, \n",
        "                                                  do_fine_tuning = do_fine_tuning, \n",
        "                                                  create_classifier = create_layers, \n",
        "                                                  bert_model_name = bert_model_name, \n",
        "                                                  folds = folds)\n",
        "\n",
        "print('Average accuracy ', np.mean(results3[bert_model_name]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "electra_base\n",
            "FOLD  0\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "383/383 [==============================] - 79s 166ms/step - loss: 0.9256 - binary_accuracy: 0.5276 - val_loss: 0.8972 - val_binary_accuracy: 0.5706\n",
            "Epoch 2/4\n",
            "383/383 [==============================] - 63s 164ms/step - loss: 0.8558 - binary_accuracy: 0.6422 - val_loss: 0.8875 - val_binary_accuracy: 0.6500\n",
            "Epoch 3/4\n",
            "383/383 [==============================] - 63s 164ms/step - loss: 0.6761 - binary_accuracy: 0.7964 - val_loss: 1.1208 - val_binary_accuracy: 0.6412\n",
            "Epoch 4/4\n",
            "383/383 [==============================] - 63s 164ms/step - loss: 0.4938 - binary_accuracy: 0.8933 - val_loss: 1.3129 - val_binary_accuracy: 0.6353\n",
            "43/43 [==============================] - 2s 50ms/step - loss: 1.3129 - binary_accuracy: 0.6353\n",
            "FOLD  1\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "383/383 [==============================] - 77s 165ms/step - loss: 0.9370 - binary_accuracy: 0.5219 - val_loss: 0.8888 - val_binary_accuracy: 0.5912\n",
            "Epoch 2/4\n",
            "383/383 [==============================] - 63s 163ms/step - loss: 0.8822 - binary_accuracy: 0.6286 - val_loss: 0.8237 - val_binary_accuracy: 0.7147\n",
            "Epoch 3/4\n",
            "383/383 [==============================] - 63s 163ms/step - loss: 0.7329 - binary_accuracy: 0.7690 - val_loss: 0.8963 - val_binary_accuracy: 0.6882\n",
            "Epoch 4/4\n",
            "383/383 [==============================] - 62s 163ms/step - loss: 0.5795 - binary_accuracy: 0.8629 - val_loss: 0.9829 - val_binary_accuracy: 0.6882\n",
            "43/43 [==============================] - 2s 50ms/step - loss: 0.9829 - binary_accuracy: 0.6882\n",
            "FOLD  2\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "383/383 [==============================] - 80s 166ms/step - loss: 0.9577 - binary_accuracy: 0.4969 - val_loss: 0.8591 - val_binary_accuracy: 0.6706\n",
            "Epoch 2/4\n",
            "383/383 [==============================] - 62s 163ms/step - loss: 0.8602 - binary_accuracy: 0.6283 - val_loss: 0.8478 - val_binary_accuracy: 0.6735\n",
            "Epoch 3/4\n",
            "383/383 [==============================] - 62s 163ms/step - loss: 0.6938 - binary_accuracy: 0.7742 - val_loss: 1.0384 - val_binary_accuracy: 0.6676\n",
            "Epoch 4/4\n",
            "383/383 [==============================] - 62s 163ms/step - loss: 0.5071 - binary_accuracy: 0.8923 - val_loss: 1.2453 - val_binary_accuracy: 0.6500\n",
            "43/43 [==============================] - 2s 50ms/step - loss: 1.2453 - binary_accuracy: 0.6500\n",
            "FOLD  3\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "383/383 [==============================] - 77s 165ms/step - loss: 0.9408 - binary_accuracy: 0.5336 - val_loss: 0.8855 - val_binary_accuracy: 0.6382\n",
            "Epoch 2/4\n",
            "383/383 [==============================] - 62s 163ms/step - loss: 0.8625 - binary_accuracy: 0.6429 - val_loss: 0.9273 - val_binary_accuracy: 0.6235\n",
            "Epoch 3/4\n",
            "383/383 [==============================] - 62s 163ms/step - loss: 0.7161 - binary_accuracy: 0.7664 - val_loss: 0.9575 - val_binary_accuracy: 0.6706\n",
            "Epoch 4/4\n",
            "383/383 [==============================] - 62s 163ms/step - loss: 0.5654 - binary_accuracy: 0.8659 - val_loss: 1.1004 - val_binary_accuracy: 0.6647\n",
            "43/43 [==============================] - 2s 50ms/step - loss: 1.1004 - binary_accuracy: 0.6647\n",
            "FOLD  4\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "383/383 [==============================] - 80s 173ms/step - loss: 0.9436 - binary_accuracy: 0.4964 - val_loss: 0.8805 - val_binary_accuracy: 0.6235\n",
            "Epoch 2/4\n",
            "383/383 [==============================] - 62s 163ms/step - loss: 0.8549 - binary_accuracy: 0.6404 - val_loss: 0.8691 - val_binary_accuracy: 0.6500\n",
            "Epoch 3/4\n",
            "383/383 [==============================] - 62s 163ms/step - loss: 0.7205 - binary_accuracy: 0.7668 - val_loss: 1.0190 - val_binary_accuracy: 0.6382\n",
            "Epoch 4/4\n",
            "383/383 [==============================] - 62s 163ms/step - loss: 0.5508 - binary_accuracy: 0.8585 - val_loss: 1.1663 - val_binary_accuracy: 0.6235\n",
            "43/43 [==============================] - 2s 50ms/step - loss: 1.1663 - binary_accuracy: 0.6235\n",
            "FOLD  5\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "383/383 [==============================] - 77s 165ms/step - loss: 0.9458 - binary_accuracy: 0.4856 - val_loss: 0.8962 - val_binary_accuracy: 0.5794\n",
            "Epoch 2/4\n",
            "383/383 [==============================] - 62s 163ms/step - loss: 0.8627 - binary_accuracy: 0.6246 - val_loss: 0.9000 - val_binary_accuracy: 0.6324\n",
            "Epoch 3/4\n",
            "383/383 [==============================] - 62s 163ms/step - loss: 0.7043 - binary_accuracy: 0.7624 - val_loss: 1.0347 - val_binary_accuracy: 0.6265\n",
            "Epoch 4/4\n",
            "383/383 [==============================] - 62s 163ms/step - loss: 0.5400 - binary_accuracy: 0.8565 - val_loss: 1.1755 - val_binary_accuracy: 0.6206\n",
            "43/43 [==============================] - 2s 51ms/step - loss: 1.1755 - binary_accuracy: 0.6206\n",
            "FOLD  6\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "383/383 [==============================] - 78s 165ms/step - loss: 0.9426 - binary_accuracy: 0.5172 - val_loss: 0.8747 - val_binary_accuracy: 0.6441\n",
            "Epoch 2/4\n",
            "383/383 [==============================] - 62s 163ms/step - loss: 0.8640 - binary_accuracy: 0.6468 - val_loss: 0.9567 - val_binary_accuracy: 0.6265\n",
            "Epoch 3/4\n",
            "383/383 [==============================] - 62s 163ms/step - loss: 0.6986 - binary_accuracy: 0.7853 - val_loss: 0.9473 - val_binary_accuracy: 0.6706\n",
            "Epoch 4/4\n",
            "383/383 [==============================] - 62s 163ms/step - loss: 0.5596 - binary_accuracy: 0.8601 - val_loss: 1.0588 - val_binary_accuracy: 0.6735\n",
            "43/43 [==============================] - 2s 50ms/step - loss: 1.0588 - binary_accuracy: 0.6735\n",
            "FOLD  7\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "383/383 [==============================] - 82s 165ms/step - loss: 0.9242 - binary_accuracy: 0.4937 - val_loss: 0.8601 - val_binary_accuracy: 0.6235\n",
            "Epoch 2/4\n",
            "383/383 [==============================] - 62s 163ms/step - loss: 0.8552 - binary_accuracy: 0.6193 - val_loss: 0.8176 - val_binary_accuracy: 0.6588\n",
            "Epoch 3/4\n",
            "383/383 [==============================] - 62s 163ms/step - loss: 0.7115 - binary_accuracy: 0.7414 - val_loss: 0.9065 - val_binary_accuracy: 0.6500\n",
            "Epoch 4/4\n",
            "383/383 [==============================] - 62s 163ms/step - loss: 0.5591 - binary_accuracy: 0.8534 - val_loss: 1.0218 - val_binary_accuracy: 0.6559\n",
            "43/43 [==============================] - 2s 50ms/step - loss: 1.0218 - binary_accuracy: 0.6559\n",
            "FOLD  8\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "383/383 [==============================] - 78s 165ms/step - loss: 0.9409 - binary_accuracy: 0.4911 - val_loss: 0.8668 - val_binary_accuracy: 0.6382\n",
            "Epoch 2/4\n",
            "383/383 [==============================] - 62s 163ms/step - loss: 0.8371 - binary_accuracy: 0.6561 - val_loss: 1.1424 - val_binary_accuracy: 0.5706\n",
            "Epoch 3/4\n",
            "383/383 [==============================] - 62s 163ms/step - loss: 0.6442 - binary_accuracy: 0.8141 - val_loss: 1.2434 - val_binary_accuracy: 0.6118\n",
            "Epoch 4/4\n",
            "383/383 [==============================] - 62s 163ms/step - loss: 0.4957 - binary_accuracy: 0.9007 - val_loss: 1.3552 - val_binary_accuracy: 0.6706\n",
            "43/43 [==============================] - 2s 50ms/step - loss: 1.3552 - binary_accuracy: 0.6706\n",
            "FOLD  9\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "383/383 [==============================] - 83s 165ms/step - loss: 0.9243 - binary_accuracy: 0.5378 - val_loss: 0.8676 - val_binary_accuracy: 0.6294\n",
            "Epoch 2/4\n",
            "383/383 [==============================] - 63s 163ms/step - loss: 0.8431 - binary_accuracy: 0.6506 - val_loss: 0.9074 - val_binary_accuracy: 0.6647\n",
            "Epoch 3/4\n",
            "383/383 [==============================] - 62s 163ms/step - loss: 0.6919 - binary_accuracy: 0.7796 - val_loss: 1.0338 - val_binary_accuracy: 0.6382\n",
            "Epoch 4/4\n",
            "383/383 [==============================] - 62s 163ms/step - loss: 0.5507 - binary_accuracy: 0.8607 - val_loss: 1.2469 - val_binary_accuracy: 0.6382\n",
            "43/43 [==============================] - 2s 50ms/step - loss: 1.2469 - binary_accuracy: 0.6382\n",
            "Average accuracy  0.6520588219165802\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SS5ttFo1fAU2"
      },
      "source": [
        "# electra with fine-tuning, best CNN \n",
        "bert_model_name = 'electra_base'\n",
        "epochs = 2\n",
        "init_lr = 2e-5\n",
        "batch_size = 8\n",
        "do_fine_tuning = True\n",
        "folds = 10\n",
        "\n",
        "def create_layers(net, text_input): \n",
        "  filter_length = 1\n",
        "  seq_len = 256\n",
        "  net = tf.keras.layers.Reshape((2,384), input_shape=(None,768))(net)\n",
        "  net = tf.keras.layers.Conv1D(filters=256, kernel_size=(filter_length), padding='valid',strides=1, activation='relu')(net)\n",
        "  net = tf.keras.layers.MaxPool1D(pool_size=2)(net)\n",
        "  net = tf.keras.layers.Flatten()(net)\n",
        "  net = tf.keras.layers.Dense(256, activation = 'relu')(net)\n",
        "  net = tf.keras.layers.Dropout(0.1)(net)\n",
        "  net = tf.keras.layers.Dense(1, activation='sigmoid', name='classifier', \n",
        "                              kernel_regularizer=tf.keras.regularizers.L1L2(l1=0.01, l2=0.02)\n",
        "                              )(net)\n",
        "  return tf.keras.Model(text_input, net)\n",
        "\n",
        "results3, history3, classifier_model3 = bert_10_fold(epochs=epochs, \n",
        "                                                  init_lr=init_lr, \n",
        "                                                  batch_size = batch_size, \n",
        "                                                  do_fine_tuning = do_fine_tuning, \n",
        "                                                  create_classifier = create_layers, \n",
        "                                                  bert_model_name = bert_model_name, \n",
        "                                                  folds = folds)\n",
        "\n",
        "print('Average accuracy ', np.mean(results3[bert_model_name]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5mm5o6gaNzX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeed4COdunkH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cG1lzQuFunuw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lf8522XPun3H",
        "outputId": "12168371-be51-4424-a9e3-e46b89d31c7b"
      },
      "source": [
        "# electra with fine-tuning, best CNN \n",
        "bert_model_name = 'electra_base'\n",
        "epochs = 4\n",
        "init_lr = 2e-5\n",
        "batch_size = 8\n",
        "do_fine_tuning = True\n",
        "folds = 1\n",
        "\n",
        "def create_layers(net, text_input): \n",
        "  filter_length = 1\n",
        "  seq_len = 256\n",
        "  net = tf.keras.layers.Reshape((2,384), input_shape=(None,768))(net)\n",
        "  net = tf.keras.layers.Conv1D(filters=256, kernel_size=(filter_length), padding='valid',strides=1, activation='relu')(net)\n",
        "  net = tf.keras.layers.MaxPool1D(pool_size=2)(net)\n",
        "  net = tf.keras.layers.Flatten()(net)\n",
        "  net = tf.keras.layers.Dense(256, activation = 'relu')(net)\n",
        "  net = tf.keras.layers.Dropout(0.1)(net)\n",
        "  net = tf.keras.layers.Dense(1, activation='sigmoid', name='classifier', \n",
        "                              kernel_regularizer=tf.keras.regularizers.L1L2(l1=0.01, l2=0.02)\n",
        "                              )(net)\n",
        "  return tf.keras.Model(text_input, net)\n",
        "\n",
        "results, history, classifier_model = bert_10_fold(epochs=epochs, \n",
        "                                                  init_lr=init_lr, \n",
        "                                                  batch_size = batch_size, \n",
        "                                                  do_fine_tuning = do_fine_tuning, \n",
        "                                                  create_classifier = create_layers, \n",
        "                                                  bert_model_name = bert_model_name, \n",
        "                                                  folds = folds)\n",
        "\n",
        "print('Average accuracy ', np.mean(results3[bert_model_name]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "electra_base\n",
            "FOLD  0\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "383/383 [==============================] - 79s 166ms/step - loss: 0.9291 - binary_accuracy: 0.5341 - val_loss: 0.8914 - val_binary_accuracy: 0.5794\n",
            "Epoch 2/4\n",
            "383/383 [==============================] - 63s 164ms/step - loss: 0.8526 - binary_accuracy: 0.6519 - val_loss: 0.8745 - val_binary_accuracy: 0.6412\n",
            "Epoch 3/4\n",
            "383/383 [==============================] - 63s 164ms/step - loss: 0.6885 - binary_accuracy: 0.7848 - val_loss: 1.1265 - val_binary_accuracy: 0.6235\n",
            "Epoch 4/4\n",
            "383/383 [==============================] - 63s 164ms/step - loss: 0.5354 - binary_accuracy: 0.8721 - val_loss: 1.2011 - val_binary_accuracy: 0.6382\n",
            "43/43 [==============================] - 2s 50ms/step - loss: 1.2011 - binary_accuracy: 0.6382\n",
            "Average accuracy  0.6382352709770203\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQ1Vo0vXwv4D",
        "outputId": "bde01cc0-7ede-4a26-c178-5c435ea5f6a4"
      },
      "source": [
        "# error analysis \n",
        "bert_model_name = 'electra_base'\n",
        "epochs = 3\n",
        "init_lr = 2e-5\n",
        "batch_size = 8\n",
        "do_fine_tuning = True\n",
        "folds = 1\n",
        "\n",
        "def create_layers(net, text_input): \n",
        "  net = tf.keras.layers.Dropout(0.1)(net)\n",
        "  net = tf.keras.layers.Dense(1, activation='sigmoid', name='classifier', \n",
        "                              kernel_regularizer=tf.keras.regularizers.L1L2(l1=0.01, l2=0.01)\n",
        "                              )(net)\n",
        "  return tf.keras.Model(text_input, net)\n",
        "\n",
        "results, history, classifier_model = bert_10_fold(epochs=epochs, \n",
        "                                                  init_lr=init_lr, \n",
        "                                                  batch_size = batch_size, \n",
        "                                                  do_fine_tuning = do_fine_tuning, \n",
        "                                                  create_classifier = create_layers, \n",
        "                                                  bert_model_name = bert_model_name, \n",
        "                                                  folds = folds)\n",
        "\n",
        "print('Average accuracy ', np.mean(results[bert_model_name]))\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "electra_base\n",
            "FOLD  0\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "383/383 [==============================] - 78s 165ms/step - loss: 1.0415 - binary_accuracy: 0.5286 - val_loss: 0.9885 - val_binary_accuracy: 0.6294\n",
            "Epoch 2/3\n",
            "383/383 [==============================] - 63s 163ms/step - loss: 0.9241 - binary_accuracy: 0.6786 - val_loss: 0.9859 - val_binary_accuracy: 0.6647\n",
            "Epoch 3/3\n",
            "383/383 [==============================] - 63s 164ms/step - loss: 0.7303 - binary_accuracy: 0.8271 - val_loss: 1.1115 - val_binary_accuracy: 0.6588\n",
            "43/43 [==============================] - 2s 51ms/step - loss: 1.1115 - binary_accuracy: 0.6588\n",
            "Average accuracy  0.658823549747467\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Kt8hM0ZyRgN",
        "outputId": "9de2ee9d-2d6f-481a-f1e2-2820ef8c86bb"
      },
      "source": [
        "# electra with fine-tuning, LSTM \n",
        "bert_model_name = 'electra_base'\n",
        "epochs = 1\n",
        "init_lr = 2e-5\n",
        "batch_size = 32\n",
        "do_fine_tuning = True\n",
        "folds = 1\n",
        "\n",
        "def create_layers(net, text_input): \n",
        "  filter_length = 1\n",
        "  seq_len = 256\n",
        "  net = tf.keras.layers.Reshape((2,384), input_shape=(None,768))(net)\n",
        "  net = tf.keras.layers.LSTM(256, return_sequences=True, activation='tanh')(net)\n",
        "  net = tf.keras.layers.Dense(256, activation = 'relu')(net)\n",
        "  net = tf.keras.layers.Dropout(0.2)(net)\n",
        "  net = tf.keras.layers.Dense(1, activation='sigmoid', name='classifier', \n",
        "                              kernel_regularizer=tf.keras.regularizers.L1L2(l1=0.01, l2=0.01)\n",
        "                              )(net)\n",
        "  return tf.keras.Model(text_input, net)\n",
        "\n",
        "results, history, classifier_model = bert_10_fold(epochs=epochs, \n",
        "                                                  init_lr=init_lr, \n",
        "                                                  batch_size = batch_size, \n",
        "                                                  do_fine_tuning = do_fine_tuning, \n",
        "                                                  create_classifier = create_layers, \n",
        "                                                  bert_model_name = bert_model_name, \n",
        "                                                  folds = folds)\n",
        "\n",
        "print('Average error ', np.mean(results[bert_model_name]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "electra_base\n",
            "FOLD  0\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "96/96 [==============================] - 70s 561ms/step - loss: 0.9007 - binary_accuracy: 0.5129 - val_loss: 0.8957 - val_binary_accuracy: 0.5794\n",
            "11/11 [==============================] - 2s 189ms/step - loss: 0.8957 - binary_accuracy: 0.5794\n",
            "Average error  0.5794117450714111\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLc5lAt1wylW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc6104b5-2790-4569-fcb7-0106c65f60f7"
      },
      "source": [
        "# electra with fine-tuning, LSTM \n",
        "bert_model_name = 'electra_base'\n",
        "epochs = 1\n",
        "init_lr = 2e-5\n",
        "batch_size = 32\n",
        "do_fine_tuning = True\n",
        "folds = 1\n",
        "\n",
        "def create_layers(net, text_input): \n",
        "  filter_length = 1\n",
        "  seq_len = 256\n",
        "  net = tf.keras.layers.Reshape((2,384), input_shape=(None,768))(net)\n",
        "  net = tf.keras.layers.LSTM(256, return_sequences=True, activation='tanh')(net)\n",
        "  net = tf.keras.layers.Dense(256, activation = 'relu')(net)\n",
        "  net = tf.keras.layers.Dropout(0.2)(net)\n",
        "  net = tf.keras.layers.Dense(1, activation='sigmoid', name='classifier', \n",
        "                              kernel_regularizer=tf.keras.regularizers.L1L2(l1=0.01, l2=0.01)\n",
        "                              )(net)\n",
        "  return tf.keras.Model(text_input, net)\n",
        "\n",
        "results, history, classifier_model = bert_10_fold(epochs=epochs, \n",
        "                                                  init_lr=init_lr, \n",
        "                                                  batch_size = batch_size, \n",
        "                                                  do_fine_tuning = do_fine_tuning, \n",
        "                                                  create_classifier = create_layers, \n",
        "                                                  bert_model_name = bert_model_name, \n",
        "                                                  folds = folds)\n",
        "\n",
        "print('Average error ', np.mean(results[bert_model_name]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:Using /tmp/tfhub_modules to cache modules.\n",
            "INFO:absl:Downloading TF-Hub Module 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "electra_base\n",
            "FOLD  0\n",
            "Train  3062\n",
            "Test  340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:Downloaded https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3, Total size: 1.96MB\n",
            "INFO:absl:Downloaded TF-Hub Module 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'.\n",
            "INFO:absl:Downloading TF-Hub Module 'https://tfhub.dev/google/electra_base/2'.\n",
            "INFO:absl:Downloaded https://tfhub.dev/google/electra_base/2, Total size: 425.70MB\n",
            "INFO:absl:Downloaded TF-Hub Module 'https://tfhub.dev/google/electra_base/2'.\n",
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "96/96 [==============================] - 111s 921ms/step - loss: 0.9042 - binary_accuracy: 0.5279 - val_loss: 0.9013 - val_binary_accuracy: 0.5882\n",
            "11/11 [==============================] - 3s 304ms/step - loss: 0.9013 - binary_accuracy: 0.5882\n",
            "Average error  0.5882353186607361\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYt9sljGvNLD"
      },
      "source": [
        "# save last model\n",
        "model_path = 'baseline_electra_ibc_reg'\n",
        "classifier_model.save(model_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sadOxoMeZVSU"
      },
      "source": [
        "# error analysis on last test set\n",
        "model_path = 'baseline_electra_ibc_reg'\n",
        "classifier_model = tf.saved_model.load(model_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "pbb8SjArx9ct",
        "outputId": "92044c68-21d9-4b5a-e7f5-dc806c3118fa"
      },
      "source": [
        "# plot change\n",
        "plot_loss(history)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['loss', 'binary_accuracy', 'val_loss', 'val_binary_accuracy'])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x432 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 758
        },
        "id": "dTtgRcmex_MT",
        "outputId": "c1e0ba83-c42c-450c-a150-956fe16829d5"
      },
      "source": [
        "# model structure\n",
        "tf.keras.utils.plot_model(classifier_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "id": "rQK9oHp58doh",
        "outputId": "ef6d6002-6ffe-4144-a0a1-1a8fbdf1760e"
      },
      "source": [
        "#shapes of inputs\n",
        "tf.keras.utils.plot_model(classifier_model, show_shapes=True, dpi=58)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HM_V1hKs7oop"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWZQLYuGc3_U"
      },
      "source": [
        "def create_test_results():\n",
        "  fold = 0\n",
        "  train, test = stratified_10_fold_split(df, fold = fold, val = False, batch_size = batch_size) \n",
        "  classifier_model.evaluate(test)\n",
        "  preds = classifier_model.predict(test)\n",
        "  pred_prob = [i[0] for i in preds]\n",
        "  pred_label = [1 if i > 0.5 else 0 for i in pred_prob]\n",
        "  sentence = []\n",
        "  actual_label = []\n",
        "  for text_batch, label_batch in test: \n",
        "    for i in range(len(text_batch)): \n",
        "      sentence.append(text_batch.numpy()[i])\n",
        "      actual_label.append(label_batch.numpy()[i])\n",
        "  test_results = pd.DataFrame(zip(sentence, actual_label, pred_label, pred_prob), \n",
        "                              columns = ['sentence', 'actual', 'pred', 'pred_prob'])\n",
        "  test_results['diff'] = [np.abs(a-b) for a,b in zip(test_results['actual'], test_results['pred_prob'])]\n",
        "  print(len(test_results[test_results.actual == test_results.pred]) / len(test_results))\n",
        "  return test_results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4Qksg-Xa3HV",
        "outputId": "9f54f217-cdb7-4a25-a3c6-396259ecfd4f"
      },
      "source": [
        "test_results = create_test_results()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train  3062\n",
            "Test  340\n",
            "11/11 [==============================] - 3s 304ms/step - loss: 0.9013 - binary_accuracy: 0.5882\n",
            "0.5970588235294118\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_etAGfdjrqIJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 580
        },
        "id": "C2irFM0Nq2uC",
        "outputId": "04741ef5-dd1c-44e2-a1df-6767402fa3f4"
      },
      "source": [
        "print('Probability Distribution')\n",
        "pd.DataFrame([i[0] for i in test_results['pred_prob']]).hist()\n",
        "plt.show()\n",
        "print('Actual vs. Predicted Labels')\n",
        "test_results['actual'].hist()\n",
        "test_results['pred'].hist()\n",
        "plt.legend(['Actual', 'Predicted'])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Probability Distribution\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Actual vs. Predicted Labels\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fcc7293dd50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "Ww2KMFobaheF",
        "outputId": "699212ca-5d3b-4da3-a77b-a84d5d07922f"
      },
      "source": [
        "print('Probability Distribution')\n",
        "test_results['pred_prob'].hist()\n",
        "plt.show()\n",
        "print('Actual vs. Predicted Labels')\n",
        "test_results['actual'].hist()\n",
        "test_results['pred'].hist()\n",
        "plt.legend(['Actual', 'Predicted'])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Probability Distribution\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Actual vs. Predicted Labels\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fae0b937550>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAa3klEQVR4nO3dfXRV9Z3v8fdXHkxLcgEBs1D0BpegBqSAB2sVuSfq5UFbqS1LoTOtjGjKXO3q1Nqr084qqGtcjh2Vil41Iy5xrAlWq1AfOlrlrPgwVIPmIpKiKNEbzQIKFHJUUPB7/zgb5oAJOdnniezzea2VlX1+++H3+56EDye/s8/e5u6IiEi0HFHsAYiISO4p3EVEIkjhLiISQQp3EZEIUriLiERQ32IPAGDo0KFeVVUVev+PP/6YAQMG5G5Ah7lSqxdUc6lQzT2zevXqv7j7sM7WHRbhXlVVRVNTU+j9E4kE8Xg8dwM6zJVavaCaS4Vq7hkze7+rdZqWERGJoG7D3cyOM7OVZrbOzN4ysx8H7UeZ2XNm9k7wfXDQbmZ2h5ltMLM1ZjYx30WIiMiBMnnlvgf4qbtXA2cAV5pZNXAd8Ly7jwKeDx4DzABGBV+1wN05H7WIiBxSt+Hu7u3u/nqw3AG0AMcCM4GlwWZLgW8HyzOBBz1lFTDIzIbnfOQiItIl68m1ZcysCmgExgIfuPugoN2A7e4+yMyeBG5295eCdc8D17p700HHqiX1yp7KysrTGhoaQheRTCYpLy8PvX9vU2r1gmouFaq5Z2pqala7e6yzdRmfLWNm5cBjwD+4+85Unqe4u5tZj65A5u51QB1ALBbzbN4hL7V32EutXlDNpUI1505GZ8uYWT9Swf4bd/9d0Lxp33RL8H1z0P4hcFza7iOCNhERKZBMzpYxYAnQ4u63pa1aAVwaLF8KLE9r/0Fw1swZwA53b8/hmEVEpBuZTMucBXwfeNPMmoO2nwM3A4+Y2TzgfeDiYN3TwPnABuAT4O9yOmIREelWt+EevDFqXaw+t5PtHbgyy3GJiBTOwoHF6zu+vPttQtAnVEVEIkjhLiISQQp3EZEIUriLiESQwl1EJIIU7iIiEaRwFxGJIIW7iEgEKdxFRCJI4S4iEkEKdxGRCFK4i4hEkMJdRCSCFO4iIhGkcBcRiSCFu4hIBCncRUQiSOEuIhJBmdwg+34z22xma9PalplZc/DVuu/eqmZWZWafpq27J5+DFxGRzmVyg+wHgDuBB/c1uPsl+5bN7FZgR9r277r7+FwNUEREei6TG2Q3mllVZ+vMzICLgXNyOywREcmGuXv3G6XC/Ul3H3tQ+xTgNnePpW33FvA2sBP4J3d/sYtj1gK1AJWVlac1NDSErYFkMkl5eXno/XubUqsXVHOpKFrN7c2F7zOQrDgxdM01NTWr9+XvwTKZljmUOUB92uN24Hh332pmpwFPmNkYd9958I7uXgfUAcRiMY/H46EHkUgkyGb/3qbU6gXVXCqKVvPCmYXvM5CIL89LzaHPljGzvsB3gGX72tx9t7tvDZZXA+8Co7MdpIiI9Ew2p0KeB/zZ3dv2NZjZMDPrEyyfAIwC3stuiCIi0lOZnApZD/wncJKZtZnZvGDVbA6ckgGYAqwJTo18FJjv7ttyOWAREeleJmfLzOmifW4nbY8Bj2U/rJ5588MdzL3uqUJ3S+vNFxS8TxGRTOgTqiIiEaRwFxGJIIW7iEgEKdxFRCJI4S4iEkEKdxGRCFK4i4hEkMJdRCSCFO4iIhGkcBcRiSCFu4hIBCncRUQiSOEuIhJBCncRkQhSuIuIRJDCXUQkghTuIiIRlMlt9u43s81mtjatbaGZfWhmzcHX+Wnr/tHMNpjZejOblq+Bi4hI1zJ55f4AML2T9tvdfXzw9TSAmVWTurfqmGCf/7PvhtkiIlI43Ya7uzcCmd7keibQ4O673X0jsAE4PYvxiYhICObu3W9kVgU86e5jg8cLgbnATqAJ+Km7bzezO4FV7v5QsN0S4Bl3f7STY9YCtQCVlZWnNTQ0hC5i87YdbPo09O6hnXrswMJ3CiSTScrLy4vSd7Go5tJQtJrbmwvfZyBZcWLommtqala7e6yzdX1Djudu4EbAg++3Apf15ADuXgfUAcRiMY/H4yGHAot/s5xb3wxbSnitfxMveJ8AiUSCbJ6v3kg1l4ai1bxwZuH7DCTiy/NSc6izZdx9k7vvdfcvgH/jv6ZePgSOS9t0RNAmIiIFFCrczWx42sOLgH1n0qwAZpvZkWY2EhgFvJrdEEVEpKe6ncsws3ogDgw1szZgARA3s/GkpmVagR8CuPtbZvYIsA7YA1zp7nvzM3QREelKt+Hu7nM6aV5yiO3/GfjnbAYlIiLZ0SdURUQiqPCnmOTBqUdspLVsQRF63lGEPkVEuqdX7iIiEaRwFxGJIIW7iEgEKdxFRCJI4S4iEkEKdxGRCFK4i4hEkMJdRCSCFO4iIhGkcBcRiSCFu4hIBCncRUQiSOEuIhJBCncRkQhSuIuIRJDCXUQkgroNdzO738w2m9natLZfmdmfzWyNmT1uZoOC9ioz+9TMmoOve/I5eBER6Vwmr9wfAKYf1PYcMNbdxwFvA/+Ytu5ddx8ffM3PzTBFRKQnug13d28Eth3U9qy77wkergJG5GFsIiISkrl79xuZVQFPuvvYTtb9Hljm7g8F271F6tX8TuCf3P3FLo5ZC9QCVFZWntbQ0BCuAiC5bTPluz8KvX9ow8cXvk8gmUxSXl5elL6LRTWXhqLV3N5c+D4DyYoTQ9dcU1Oz2t1jna3L6gbZZvYLYA/wm6CpHTje3bea2WnAE2Y2xt13Hryvu9cBdQCxWMzj8XjocSTqFxFfX4QbZM8pzg2yE4kE2TxfvZFqLg1Fq3nhzML3GUjEl+el5tBny5jZXOCbwN948PLf3Xe7+9ZgeTXwLjA6B+MUEZEeCBXuZjYd+N/Ahe7+SVr7MDPrEyyfAIwC3svFQEVEJHPdTsuYWT0QB4aaWRuwgNTZMUcCz5kZwKrgzJgpwA1m9jnwBTDf3bd1emAREcmbbsPd3ed00ryki20fAx7LdlAiIpIdfUJVRCSCFO4iIhGkcBcRiSCFu4hIBCncRUQiSOEuIhJBCncRkQhSuIuIRJDCXUQkghTuIiIRpHAXEYkghbuISAQp3EVEIkjhLiISQQp3EZEIUriLiESQwl1EJIIyCnczu9/MNpvZ2rS2o8zsOTN7J/g+OGg3M7vDzDaY2Rozm5ivwYuISOcyfeX+ADD9oLbrgOfdfRTwfPAYYAapG2OPAmqBu7MfpoiI9ERG4e7ujcDBN7qeCSwNlpcC305rf9BTVgGDzGx4LgYrIiKZMXfPbEOzKuBJdx8bPP6ruw8Klg3Y7u6DzOxJ4GZ3fylY9zxwrbs3HXS8WlKv7KmsrDytoaEhdBHJbZsp3/1R6P1DGz6+8H0CyWSS8vLyovRdLKq5NBSt5vbmwvcZSFacGLrmmpqa1e4e62xd36xGFXB3N7PM/pf4r33qgDqAWCzm8Xg8dP+J+kXE1y8IvX9oc3YUvk8gkUiQzfPVG6nm0lC0mhfOLHyfgUR8eV5qzuZsmU37pluC75uD9g+B49K2GxG0iYhIgWQT7iuAS4PlS4Hlae0/CM6aOQPY4e7tWfQjIiI9lNG0jJnVA3FgqJm1AQuAm4FHzGwe8D5wcbD508D5wAbgE+DvcjxmERHpRkbh7u5zulh1bifbOnBlNoMSEZHs6BOqIiIRpHAXEYkghbuISAQp3EVEIkjhLiISQQp3EZEIUriLiESQwl1EJIIU7iIiEaRwFxGJIIW7iEgEKdxFRCJI4S4iEkEKdxGRCFK4i4hEkMJdRCSCFO4iIhGU0Z2YOmNmJwHL0ppOAH4JDAKuALYE7T9396dDj1BERHosdLi7+3pgPICZ9QE+BB4ndc/U2939X3MyQhER6bFcTcucC7zr7u/n6HgiIpKFXIX7bKA+7fFVZrbGzO43s8E56kNERDJk7p7dAcz6Ax8BY9x9k5lVAn8BHLgRGO7ul3WyXy1QC1BZWXlaQ0ND6DEkt22mfPdHofcPbfj4wvcJJJNJysvLi9J3sajm0lC0mtubC99nIFlxYuiaa2pqVrt7rLN1uQj3mcCV7j61k3VVwJPuPvZQx4jFYt7U1BR6DIn6RcTXLwi9f2gLdxS+TyCRSBCPx4vSd7Go5tJQtJoXDix8n4FEfHnoms2sy3DPxbTMHNKmZMxseNq6i4C1OehDRER6IPTZMgBmNgD4n8AP05pvMbPxpKZlWg9aJyIiBZBVuLv7x8CQg9q+n9WIREQka/qEqohIBCncRUQiSOEuIhJBCncRkQhSuIuIRJDCXUQkghTuIiIRpHAXEYkghbuISAQp3EVEIkjhLiISQQp3EZEIUriLiESQwl1EJIIU7iIiEaRwFxGJIIW7iEgEKdxFRCIoq9vsAZhZK9AB7AX2uHvMzI4ClgFVpO6jerG7b8+2LxERyUyuXrnXuPt4d48Fj68Dnnf3UcDzwWMRESmQfE3LzASWBstLgW/nqR8REemEuXt2BzDbCGwHHLjX3evM7K/uPihYb8D2fY/T9qsFagEqKytPa2hoCD2G5LbNlO/+KPT+oQ0fX/g+gWQySXl5eVH6LhbVXBqKVnN7c+H7DCQrTgxdc01Nzeq0GZMDZD3nDkx29w/N7GjgOTP7c/pKd3cz+9L/IO5eB9QBxGIxj8fjoQeQqF9EfP2C0PuHNmdH4fsEEokE2TxfvZFqLg1Fq3nhzML3GUjEl+el5qynZdz9w+D7ZuBx4HRgk5kNBwi+b862HxERyVxW4W5mA8ysYt8yMBVYC6wALg02uxRYnk0/IiLSM9lOy1QCj6em1ekLPOzufzCz14BHzGwe8D5wcZb9iIhID2QV7u7+HvC1Ttq3Audmc2wREQlPn1AVEYkghbuISAQp3EVEIkjhLiISQQp3EZEIUriLiESQwl1EJIIU7iIiEaRwFxGJoFxcFVIklM8//5y2tjZ27drV7bYDBw6kpaWlAKM6fGRac1lZGSNGjKBfv34FGJX0Fgp3KZq2tjYqKiqoqqoiuD5Rlzo6OqioqCjQyA4PmdTs7mzdupW2tjZGjhxZoJFJb6BpGSmaXbt2MWTIkG6DXbpmZgwZMiSjv36ktCjcpagU7NnTcyidUbiLiESQ5tzlsFF13VM5PV7rzRdktN0TTzzBRRddREtLCyeffHKX2y1atIja2lq++tWvhhrPAw88QFNTE3feeWeo/UV6Qq/cpeTV19czefJk6uvrD7ndokWL+OSTTwo0KpHsKNylpCWTSV566SWWLFlCQ0MDAHv37uWaa65h7NixjBs3jsWLF3PHHXfw0UcfUVNTQ01NDcABd6x/9NFHmTt3LgC///3v+frXv86ECRM477zz2LRpU8HrEgk9LWNmxwEPkrrVngN17v5rM1sIXAFsCTb9ubs/ne1ARfJh+fLlTJ8+ndGjRzNkyBBWr17Nq6++SmtrK83NzfTt25dt27Zx1FFHcdttt7Fy5UqGDh16yGNOnjyZVatWYWbcd9993HLLLdx6660FqkgkJZs59z3AT9399eAm2avN7Llg3e3u/q/ZD08kv+rr6/nxj38MwOzZs6mvr2fjxo3Mnz+fvn1T/zyOOuqoHh2zra2NSy65hPb2dj777DOdfy5FETrc3b0daA+WO8ysBTg2VwMTybdt27bxwgsv8Oabb2Jm7N27FzNj0qRJGe2ffgpi+nnmP/rRj7j66qu58MILSSQSLFy4MNdDF+lWTs6WMbMqYALwJ+As4Coz+wHQROrV/fZO9qkFagEqKytJJBKh+08eeQyJk64PvX9oWYw5G8lkMqvn63AxcOBAOjo68nb87o790EMPMXv2bH7961/vb5sxYwYnn3wyd911F7FY7IBpmQEDBtDe3s6RRx4JwLBhw2hqamLUqFH89re/pby8nI6ODrZv386gQYPo6OjgvvvuY+/evXR0dLBr1y4+++yzjGvet18mdu3aFYnfiaL9bhcjPwL5qjnrcDezcuAx4B/cfaeZ3Q3cSGoe/kbgVuCyg/dz9zqgDiAWi3k8Hg89hkT9IuLrF4TeP7Q5OwrfJ5BIJMjm+TpctLS0HPDx+kOdupiPyw88/vjjXHvttQcc9+KLL6alpYUTTjiBs846i379+nHFFVdw1VVXMX/+fGbNmsUxxxzDypUrueWWW7jkkksYNmwYsViMZDJJRUUFN9xwA3PnzmXw4MGcc845+y+zUFZWRv/+/TOuoyc1l5WVMWHChFDPw+GkaL/bC2cWvs9AIr48LzWbu4ff2awf8CTwH+5+Wyfrq4An3X3soY4Ti8W8qakp9DiKFu4LFe7ZaGlp4ZRTTsloW11b5tB68lwezooX7gML32cgm3A3s9XuHutsXehTIS014bgEaEkPdjMbnrbZRcDasH2IiEg42UzLnAV8H3jTzJqDtp8Dc8xsPKlpmVbgh1mNUEREeiybs2VeAjq7YpHOaRcRKTJ9QlVEJIIU7iIiEaRwFxGJIF3yVw4fhzgdLdRJkBmcqtqnTx9OPfVU9uzZwymnnMLSpUtDX9J37ty5fPOb32TWrFlcfvnlXH311VRXV3e6bSKRoH///px55pk96qOqqoqmpqZur28jolfuUtK+8pWv0NzczNq1a+nfvz/33HPPAev37NkT6rj33Xdfl8EOqXB/5ZVXQh1bJBMKd5HA2WefzYYNG0gkEpx99tlceOGFVFdXs3fvXn72s58xadIkxo0bx7333gukbk591VVXcdJJJ3HeeeexefPm/ceKx+Ps+2DeH/7wByZOnMjXvvY1zj33XFpbW7nnnnu4/fbbGT9+PC+++CJbtmzhu9/9LpMmTWLSpEm8/PLLAGzdupWpU6cyZswYLr/8crL50KGUFk3LiJB6hf7MM88wffp0AF5//XXWrl3LyJEjqaurY+DAgbz22mvs3r2bs846i6lTp/LGG2+wfv161q1bx6ZNm6iuruayyw680saWLVu44ooraGxsZOTIkfuvUzN//nzKy8u55pprAPje977HT37yEyZPnswHH3zAtGnTePXVV7n++uuZPHkyv/zlL3nqqadYsmRJwZ8b6Z0U7lLSPv30U8aPHw+kXrnPmzePV155hdNPP33/pXqfffZZ1qxZw6OPPgrAjh07eOedd2hsbGTOnDn06dOHY445hnPOOedLx1+1ahVTpkzZf6yuLh/8xz/+kXXr1u1/vHPnTpLJJI2Njfzud78D4IILLmDw4MG5K14iTeEuJW3fnPvBBgwYsH/Z3Vm8eDHTpk07YJunn87d5/W++OILVq1aRVlZ2f62fF4xU6JPc+4i3Zg2bRp33303n3/+OQBvv/02H3/8MVOmTGHZsmXs3buX9vZ2Vq5c+aV9zzjjDBobG9m4cSOQuoY8QEVFxQHhPXXqVBYvXrz/8b7/cKZMmcLDDz8MwDPPPMP27V+6erZIp/TKXQ4fhzh1sZhXhbz88stpbW1l4sSJuDvDhg3jiSee4KKLLuKFF16gurqa448/nm984xtf2nfYsGHU1dXxne98hy+++IKjjz6a5557jm9961vMmjWL5cuX779H65VXXsm4cePYs2cPU6ZM4Ve/+hULFixgzpw5jBkzhjPPPJPjjz++CM+A9EYKdylpyWTyS23xePyAS7AeccQR3HTTTdx0001f2vbOO+/s9LjpN1+YMWMGM2bMOGD96NGjWbNmzQFty5YtO+BxR0cHQ4YM4dlnn+2uDJEv0bSMiEgEKdxFRCJI4S5FpQ/lZE/PoXRG4S5FU1ZWxtatWxVOWXB3tm7desAplCKgN1SliEaMGEFbWxtbtmzpdttdu3aVXIBlWnNZWRkjRowowIikN1G4S9H069dv/yc3u5NIJJgwYUKeR3R4KcWaJXfyNi1jZtPNbL2ZbTCz6/LVj4iIfFlewt3M+gB3ATOAalI3ze76+qciIpJT+Xrlfjqwwd3fc/fPgAZgZp76EhGRg+Rrzv1Y4P+lPW4Dvp6+gZnVArXBw6SZrc+iv6HAX7LYP5zrreBdBopTb3Gp5tJQejVfX5NNzf+9qxVFe0PV3euAulwcy8ya3D2Wi2P1BqVWL6jmUqGacydf0zIfAselPR4RtImISAHkK9xfA0aZ2Ugz6w/MBlbkqS8RETlIXqZl3H2PmV0F/AfQB7jf3d/KR1+BnEzv9CKlVi+o5lKhmnPE9NFvEZHo0bVlREQiSOEuIhJBvSbcu7ucgZkdaWbLgvV/MrOqwo8ytzKo+WozW2dma8zseTPr8pzX3iLTy1aY2XfNzM2s1582l0nNZnZx8LN+y8weLvQYcy2D3+3jzWylmb0R/H6fX4xx5oqZ3W9mm81sbRfrzczuCJ6PNWY2MetO3f2w/yL1puy7wAlAf+D/AtUHbfO/gHuC5dnAsmKPuwA11wBfDZb/vhRqDrarABqBVUCs2OMuwM95FPAGMDh4fHSxx12AmuuAvw+Wq4HWYo87y5qnABOBtV2sPx94BjDgDOBP2fbZW165Z3I5g5nA0mD5UeBcMyvaR0hzoNua3X2lu38SPFxF6vMEvVmml624EfgXYFchB5cnmdR8BXCXu28HcPfNBR5jrmVSswP/LVgeCHxUwPHlnLs3AtsOsclM4EFPWQUMMrPh2fTZW8K9s8sZHNvVNu6+B9gBDCnI6PIjk5rTzSP1P39v1m3NwZ+rx7n7U4UcWB5l8nMeDYw2s5fNbJWZTS/Y6PIjk5oXAn9rZm3A08CPCjO0ounpv/du6XruEWBmfwvEgP9R7LHkk5kdAdwGzC3yUAqtL6mpmTipv84azexUd/9rUUeVX3OAB9z9VjP7BvDvZjbW3b8o9sB6i97yyj2Tyxns38bM+pL6U25rQUaXHxldwsHMzgN+AVzo7rsLNLZ86a7mCmAskDCzVlJzkyt6+Zuqmfyc24AV7v65u28E3iYV9r1VJjXPAx4BcPf/BMpIXVQsqnJ+yZbeEu6ZXM5gBXBpsDwLeMGDdyp6qW5rNrMJwL2kgr23z8NCNzW7+w53H+ruVe5eRep9hgvdvak4w82JTH63nyD1qh0zG0pqmua9Qg4yxzKp+QPgXAAzO4VUuHd/P8beawXwg+CsmTOAHe7entURi/0ucg/ebT6f1CuWd4FfBG03kPrHDakf/m+BDcCrwAnFHnMBav4jsAloDr5WFHvM+a75oG0T9PKzZTL8ORup6ah1wJvA7GKPuQA1VwMvkzqTphmYWuwxZ1lvPdAOfE7qL7F5wHxgftrP+K7g+XgzF7/XuvyAiEgE9ZZpGRER6QGFu4hIBCncRUQiSOEuIhJBCncRkQhSuIuIRJDCXUQkgv4/MiZE4HTEWIkAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "id": "Uz3_r0YxZ2sB",
        "outputId": "b40ab159-aaf1-497f-d2d6-e169a500f330"
      },
      "source": [
        "test_results.sort_values('diff', ascending = False).head(20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>actual</th>\n",
              "      <th>pred</th>\n",
              "      <th>pred_prob</th>\n",
              "      <th>diff</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>258</th>\n",
              "      <td>b'Third , a range of public policies helped fa...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.992748</td>\n",
              "      <td>0.992748</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>b'He will create flexible education accounts t...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.992496</td>\n",
              "      <td>0.992496</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>b\"Calling fatherlessness a `` social evil '' a...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.990194</td>\n",
              "      <td>0.990194</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>205</th>\n",
              "      <td>b'The wealthy do often use their wealth for po...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.986282</td>\n",
              "      <td>0.986282</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83</th>\n",
              "      <td>b'As our worst economic crisis since the Depre...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.983511</td>\n",
              "      <td>0.983511</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77</th>\n",
              "      <td>b'For the new fundamentalists , the old balanc...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.021978</td>\n",
              "      <td>0.978022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>251</th>\n",
              "      <td>b'Progressives also could have been demanding ...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.975942</td>\n",
              "      <td>0.975942</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>b'And , not surprisingly given the extent to w...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.975428</td>\n",
              "      <td>0.975428</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>269</th>\n",
              "      <td>b\"One recent editorial concluded that `` the o...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.973576</td>\n",
              "      <td>0.973576</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>b'Since politics is about shaping government p...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.027174</td>\n",
              "      <td>0.972826</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>218</th>\n",
              "      <td>b'Taxes , argued Douthat , are set to rise in ...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.968622</td>\n",
              "      <td>0.968622</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>169</th>\n",
              "      <td>b\"The fledgling conservative movement was able...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.032544</td>\n",
              "      <td>0.967456</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119</th>\n",
              "      <td>b\"If they succeed imposing their regulatory re...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.967361</td>\n",
              "      <td>0.967361</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148</th>\n",
              "      <td>b'Not least , they recorded their rage with th...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.962495</td>\n",
              "      <td>0.962495</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>200</th>\n",
              "      <td>b'Ever since the McCain-Feingold campaign-fina...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.961502</td>\n",
              "      <td>0.961502</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>224</th>\n",
              "      <td>b\"The biggest reason why Bush 's Medicare drug...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.039432</td>\n",
              "      <td>0.960568</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>124</th>\n",
              "      <td>b'I believe Soros and hundreds of other hedge ...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.957996</td>\n",
              "      <td>0.957996</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>b'It is certain that there is no way to reach ...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.044025</td>\n",
              "      <td>0.955975</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>187</th>\n",
              "      <td>b\"The December 4 Independent Mail of Anderson ...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.044178</td>\n",
              "      <td>0.955822</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>160</th>\n",
              "      <td>b\"He has considerable personal advantages over...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.950561</td>\n",
              "      <td>0.950561</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              sentence  ...      diff\n",
              "258  b'Third , a range of public policies helped fa...  ...  0.992748\n",
              "24   b'He will create flexible education accounts t...  ...  0.992496\n",
              "29   b\"Calling fatherlessness a `` social evil '' a...  ...  0.990194\n",
              "205  b'The wealthy do often use their wealth for po...  ...  0.986282\n",
              "83   b'As our worst economic crisis since the Depre...  ...  0.983511\n",
              "77   b'For the new fundamentalists , the old balanc...  ...  0.978022\n",
              "251  b'Progressives also could have been demanding ...  ...  0.975942\n",
              "14   b'And , not surprisingly given the extent to w...  ...  0.975428\n",
              "269  b\"One recent editorial concluded that `` the o...  ...  0.973576\n",
              "13   b'Since politics is about shaping government p...  ...  0.972826\n",
              "218  b'Taxes , argued Douthat , are set to rise in ...  ...  0.968622\n",
              "169  b\"The fledgling conservative movement was able...  ...  0.967456\n",
              "119  b\"If they succeed imposing their regulatory re...  ...  0.967361\n",
              "148  b'Not least , they recorded their rage with th...  ...  0.962495\n",
              "200  b'Ever since the McCain-Feingold campaign-fina...  ...  0.961502\n",
              "224  b\"The biggest reason why Bush 's Medicare drug...  ...  0.960568\n",
              "124  b'I believe Soros and hundreds of other hedge ...  ...  0.957996\n",
              "97   b'It is certain that there is no way to reach ...  ...  0.955975\n",
              "187  b\"The December 4 Independent Mail of Anderson ...  ...  0.955822\n",
              "160  b\"He has considerable personal advantages over...  ...  0.950561\n",
              "\n",
              "[20 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMiMIk6bWm9N",
        "outputId": "d5f47b09-9e6b-44d8-aff8-5fd19d8b443d"
      },
      "source": [
        "[sentence for sentence in test_results.sort_values('diff', ascending = False).head(20)['sentence']]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[b'Third , a range of public policies helped facilitate this economic mobility and opportunity : a strong minimum wage , grants for low-income students to go to college , generously subsidized state college tuition , a reliable unemployment insurance system , enforcement of the right to join a union , major incentives for homeownership , and a solid safety net for those falling on hard times .',\n",
              " b'He will create flexible education accounts that workers can use to retrain , provide retraining assistance for workers in sectors of the economy vulnerable to dislocation before they lose their jobs , and provide additional assistance for workers to afford health care .',\n",
              " b\"Calling fatherlessness a `` social evil '' and the `` engine driving our worst social problems , '' they promote father presence as a panacea for poverty , failure in school , emotional and behavioral problems among boys , premarital sex and pregnancy among girls , suicide , child abuse , and even social inequality .\",\n",
              " b'The wealthy do often use their wealth for political influence , but his argument treats the rich as if they were a unified voting block with identical interests .',\n",
              " b'As our worst economic crisis since the Depression enters its third full year , the history of the unemployed movement offers a lesson of how local and temporary struggles during a period of overall working-class retreat can interact with radical organizations to strengthen the left for larger struggles to come .',\n",
              " b'For the new fundamentalists , the old balance of morality has been abolished in favor of a new absolutism : not only is all abortion at any stage equally immoral , it is always as immoral as killing an adult human being .',\n",
              " b'Progressives also could have been demanding redress , marching for jobs , barking at the banks , and thereby attracting millions of supporters , but most were peaceably getting to know the new administration , muting their criticisms of Wall Street , and hoping the stimulus bill would work .',\n",
              " b'And , not surprisingly given the extent to which affluent men safely ensconced in liberal urban centers dominate the liberal pundit class , the arguments also greatly understate or ignore the stark class and geographic inequities in abortion access that would inevitably manifest themselves in a post-Roe world .',\n",
              " b\"One recent editorial concluded that `` the overall tax burden grew more progressive from 1979 to 1999 '' and insisted that any higher taxes on the rich would be unfair since the put-upon economic elite `` already bear an outsized share of the American tax burden .\",\n",
              " b'Since politics is about shaping government policy , and government policy is backed up by force or the threat of force , you might think that the consistent pacifist would avoid politics ; but many Christian pacifists are in politics up to their eyeballs .',\n",
              " b'Taxes , argued Douthat , are set to rise in 2013 when the Bush tax cuts expire , making at least a partial tax hike nearly certain if Obama wins reelection .',\n",
              " b\"The fledgling conservative movement was able to nominate Goldwater only because the Republican establishment was caught by surprise , and the movement still had no way to win national elections : Goldwater went down to humiliating defeat . ''\",\n",
              " b\"If they succeed imposing their regulatory regime , by 2050 they will have had no effect on the Earth 's climate -- which will have scarcely changed from the climate experienced today -- but the people enjoying the fine , mid-century weather will likely be poor , unemployed , and hungry as a result .\",\n",
              " b'Not least , they recorded their rage with the nearly $ 500 billion of the $ 787 billion stimulus package spent by Congress in tax cuts and subsidies that have yet to trickle down to produce jobs .',\n",
              " b'Ever since the McCain-Feingold campaign-finance legislation passed , Democrats have looked to so-called 527 groups-named after a part of the tax code that allows groups to raise unlimited sums and make independent expenditures on issues-to save them from cash shortfalls ( specifically for advertising ) that resulted from the reforms .',\n",
              " b\"The biggest reason why Bush 's Medicare drug benefit is so expensive is that the legislation creating it explicitly prohibits the federal government from using its bargaining power as the world 's largest buyer of drugs to negotiate lower prices from pharmaceutical companies .\",\n",
              " b'I believe Soros and hundreds of other hedge fund traders , most of them MFA members , manipulated the system using , primarily , changes in two rules that enabled them to raid the financial markets , generating huge profits for themselves , causing the financial crash of 2008 , and enabling Barack Obama to be elected .',\n",
              " b'It is certain that there is no way to reach a truly harmonious civilization with an economic system in which decisions are made by private individuals based on how much capital will be accumulated as well as personal greed and consumerism .',\n",
              " b\"The December 4 Independent Mail of Anderson , South Carolina , reported on how the Anderson County Sheriff 's Office turned something bad into something good by using seized drug money to pay the strongman 's fees to bring a message of hope , dreams , and choosing the right path to nine elementary and junior-high schools in the area .\",\n",
              " b\"He has considerable personal advantages over Paul -- eight years of executive experience , greater campaign-trail energy , better communication skills -- and today 's GOP is dominated by talk of the poor economy and the size of government , providing a far better climate for a strict libertarian than the one that prevailed in 2008 .\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOC7E7aQXO9Z"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImCd6_OBXtR8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSCnB63Xu4qe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSpZu-qFJhyn"
      },
      "source": [
        "# save model\n",
        "model_path = 'baseline_electra_ibc'\n",
        "saved_model_path = './baseline_electra_ibc_bert'.format(dataset_name.replace('/', '_'))\n",
        "\n",
        "classifier_model.save(saved_model_path)\n",
        "\n",
        "model.save('path/to/location')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yxv9U5QxJ1_P"
      },
      "source": [
        "# reload saved model\n",
        "reloaded_model = tf.saved_model.load(saved_model_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5CENZAfJ324"
      },
      "source": [
        "# test on new input\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjaekAgZhk0G"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTziycOorGfF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzUv5vjEIHku"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5a99tTrI5xS"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBps52A7gOUf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7BhcN3YXlC-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}